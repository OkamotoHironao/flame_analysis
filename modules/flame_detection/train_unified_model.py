#!/usr/bin/env python3
"""
è¤‡æ•°ãƒˆãƒ”ãƒƒã‚¯çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

è¤‡æ•°ã®ãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã€
æ±åŒ–ã—ãŸç‚ä¸Šæ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹

Usage:
    python train_unified_model.py
    python train_unified_model.py --topics æ¾æœ¬äººå¿—,ä¸‰è‹«,å¯¿å¸ãƒšãƒ­
    python train_unified_model.py --output outputs/unified_model/
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix
)
import xgboost as xgb
import joblib

# ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ï¼ˆãƒˆãƒ”ãƒƒã‚¯éä¾å­˜ï¼‰
FEATURE_COLUMNS = [
    # æŠ•ç¨¿é‡é–¢é€£
    'volume',
    'delta_volume',
    'volume_ratio',
    
    # æ„Ÿæƒ…é–¢é€£ï¼ˆBERTï¼‰
    'positive_rate',
    'neutral_rate', 
    'negative_rate',
    'delta_positive_rate',
    'delta_neutral_rate',
    'delta_negative_rate',
    
    # ç«‹å ´é–¢é€£
    'stance_favor_rate',
    'stance_against_rate',
    'stance_neutral_rate',
    
    # æ„Ÿæƒ…ç¢ºç‡ï¼ˆå¹³å‡ï¼‰
    'avg_score',
]


def discover_topics(base_dir):
    """åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯ï¼ˆãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‚‚ã®ï¼‰ã‚’æ¤œå‡º"""
    topics = []
    outputs_dir = Path(base_dir) / "outputs"
    
    if outputs_dir.exists():
        for topic_dir in outputs_dir.iterdir():
            if topic_dir.is_dir():
                labeled_csv = topic_dir / f"{topic_dir.name}_labeled.csv"
                if labeled_csv.exists():
                    topics.append(topic_dir.name)
    
    return sorted(topics)


def load_topic_data(topic_name, base_dir):
    """ãƒˆãƒ”ãƒƒã‚¯ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    csv_path = Path(base_dir) / "outputs" / topic_name / f"{topic_name}_labeled.csv"
    
    if not csv_path.exists():
        print(f"  âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {csv_path}")
        return None
    
    df = pd.read_csv(csv_path)
    
    # ãƒˆãƒ”ãƒƒã‚¯åˆ—ã‚’è¿½åŠ ï¼ˆåˆ†æç”¨ï¼‰
    df['topic'] = topic_name
    
    return df


def prepare_features(df, feature_columns):
    """ç‰¹å¾´é‡ã‚’æº–å‚™ï¼ˆæ¬ æå€¤å‡¦ç†ãªã©ï¼‰"""
    # å¿…è¦ãªåˆ—ã®ã¿æŠ½å‡º
    available_cols = [col for col in feature_columns if col in df.columns]
    missing_cols = [col for col in feature_columns if col not in df.columns]
    
    if missing_cols:
        print(f"  âš ï¸ æ¬ æç‰¹å¾´é‡: {missing_cols}")
    
    X = df[available_cols].copy()
    
    # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹ï¼ˆã¾ãŸã¯ä¸­å¤®å€¤ãªã©ï¼‰
    X = X.fillna(0)
    
    # ç„¡é™å¤§ã‚’ç½®æ›
    X = X.replace([np.inf, -np.inf], 0)
    
    return X, available_cols


def train_model(X_train, y_train, X_test, y_test, class_weight=None):
    """XGBoostãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    
    # ã‚¯ãƒ©ã‚¹é‡ã¿ã®è¨ˆç®—
    if class_weight == 'balanced':
        n_neg = (y_train == 0).sum()
        n_pos = (y_train == 1).sum()
        scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0
    else:
        scale_pos_weight = 1.0
    
    print(f"  ã‚¯ãƒ©ã‚¹é‡ã¿ (scale_pos_weight): {scale_pos_weight:.2f}")
    
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.1,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )
    
    # å­¦ç¿’
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=False
    )
    
    return model


def evaluate_model(model, X_test, y_test, topic_info=None):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1': f1_score(y_test, y_pred, zero_division=0),
    }
    
    # ROC-AUCã¯ä¸¡ã‚¯ãƒ©ã‚¹ãŒå¿…è¦
    if len(np.unique(y_test)) > 1:
        metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba)
    else:
        metrics['roc_auc'] = None
    
    return metrics, y_pred, y_pred_proba


def cross_validate_model(X, y, n_splits=5):
    """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§è©•ä¾¡"""
    # ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—
    n_neg = (y == 0).sum()
    n_pos = (y == 1).sum()
    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0
    
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.1,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )
    
    # Stratified K-Fold
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    scores = {
        'accuracy': cross_val_score(model, X, y, cv=cv, scoring='accuracy'),
        'f1': cross_val_score(model, X, y, cv=cv, scoring='f1'),
        'roc_auc': cross_val_score(model, X, y, cv=cv, scoring='roc_auc'),
    }
    
    return scores


def main():
    parser = argparse.ArgumentParser(
        description='è¤‡æ•°ãƒˆãƒ”ãƒƒã‚¯çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¾‹:
  python train_unified_model.py
  python train_unified_model.py --topics æ¾æœ¬äººå¿—,ä¸‰è‹«
  python train_unified_model.py --output outputs/unified_model/
        """
    )
    
    parser.add_argument(
        '--topics',
        type=str,
        default=None,
        help='ä½¿ç”¨ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ã€‚çœç•¥æ™‚ã¯å…¨ã¦ä½¿ç”¨'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='outputs/unified_model/',
        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
    )
    
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.2ï¼‰'
    )
    
    parser.add_argument(
        '--cv',
        type=int,
        default=5,
        help='ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ†å‰²æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰'
    )
    
    args = parser.parse_args()
    
    base_dir = Path(__file__).parent
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("ğŸ”¥ è¤‡æ•°ãƒˆãƒ”ãƒƒã‚¯çµ±åˆå­¦ç¿’")
    print("=" * 60)
    
    # ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º
    available_topics = discover_topics(base_dir)
    print(f"\nğŸ“‚ åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯: {available_topics}")
    
    if args.topics:
        target_topics = [t.strip() for t in args.topics.split(',')]
        # å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        invalid = [t for t in target_topics if t not in available_topics]
        if invalid:
            print(f"âŒ å­˜åœ¨ã—ãªã„ãƒˆãƒ”ãƒƒã‚¯: {invalid}")
            sys.exit(1)
    else:
        target_topics = available_topics
    
    print(f"ğŸ“Š ä½¿ç”¨ãƒˆãƒ”ãƒƒã‚¯: {target_topics}")
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    print(f"\n{'='*60}")
    print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»çµ±åˆ")
    print("=" * 60)
    
    all_dfs = []
    topic_stats = {}
    
    for topic in target_topics:
        print(f"\n  ğŸ“ {topic}")
        df = load_topic_data(topic, base_dir)
        
        if df is not None:
            n_total = len(df)
            n_pos = (df['is_controversy'] == 1).sum()
            n_neg = (df['is_controversy'] == 0).sum()
            
            print(f"    ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {n_total}")
            print(f"    ç‚ä¸Š(1): {n_pos} ({n_pos/n_total*100:.1f}%)")
            print(f"    éç‚ä¸Š(0): {n_neg} ({n_neg/n_total*100:.1f}%)")
            
            topic_stats[topic] = {
                'total': n_total,
                'positive': n_pos,
                'negative': n_neg
            }
            
            all_dfs.append(df)
    
    if not all_dfs:
        print("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    # çµ±åˆ
    combined_df = pd.concat(all_dfs, ignore_index=True)
    print(f"\nâœ“ çµ±åˆå®Œäº†: {len(combined_df)}ä»¶")
    
    # å…¨ä½“ã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
    total_pos = (combined_df['is_controversy'] == 1).sum()
    total_neg = (combined_df['is_controversy'] == 0).sum()
    print(f"  ç‚ä¸Š(1): {total_pos} ({total_pos/len(combined_df)*100:.1f}%)")
    print(f"  éç‚ä¸Š(0): {total_neg} ({total_neg/len(combined_df)*100:.1f}%)")
    
    if total_pos == 0 or total_neg == 0:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ä¸¡ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¿…è¦ã§ã™")
        print("   ç‚ä¸Š(1)ã¨éç‚ä¸Š(0)ã®ä¸¡æ–¹ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        sys.exit(1)
    
    # ç‰¹å¾´é‡æº–å‚™
    print(f"\n{'='*60}")
    print("ğŸ”§ ç‰¹å¾´é‡æº–å‚™")
    print("=" * 60)
    
    X, used_features = prepare_features(combined_df, FEATURE_COLUMNS)
    y = combined_df['is_controversy'].values
    
    print(f"  ä½¿ç”¨ç‰¹å¾´é‡: {len(used_features)}å€‹")
    for f in used_features:
        print(f"    - {f}")
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    print(f"\n{'='*60}")
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²")
    print("=" * 60)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y,
        test_size=args.test_size,
        random_state=42,
        stratify=y
    )
    
    print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶")
    print(f"    ç‚ä¸Š(1): {(y_train==1).sum()}")
    print(f"    éç‚ä¸Š(0): {(y_train==0).sum()}")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ä»¶")
    print(f"    ç‚ä¸Š(1): {(y_test==1).sum()}")
    print(f"    éç‚ä¸Š(0): {(y_test==0).sum()}")
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    print(f"\n{'='*60}")
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
    print("=" * 60)
    
    model = train_model(X_train, y_train, X_test, y_test, class_weight='balanced')
    
    # è©•ä¾¡
    print(f"\n{'='*60}")
    print("ğŸ“ˆ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
    print("=" * 60)
    
    metrics, y_pred, y_pred_proba = evaluate_model(model, X_test, y_test)
    
    print(f"\n  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡:")
    print(f"    Accuracy:  {metrics['accuracy']*100:.2f}%")
    print(f"    Precision: {metrics['precision']*100:.2f}%")
    print(f"    Recall:    {metrics['recall']*100:.2f}%")
    print(f"    F1 Score:  {metrics['f1']*100:.2f}%")
    if metrics['roc_auc']:
        print(f"    ROC-AUC:   {metrics['roc_auc']*100:.2f}%")
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(y_test, y_pred)
    print(f"\n  æ··åŒè¡Œåˆ—:")
    print(f"              äºˆæ¸¬:éç‚ä¸Š  äºˆæ¸¬:ç‚ä¸Š")
    print(f"    å®Ÿéš›:éç‚ä¸Š    {cm[0,0]:4d}      {cm[0,1]:4d}")
    print(f"    å®Ÿéš›:ç‚ä¸Š      {cm[1,0]:4d}      {cm[1,1]:4d}")
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    print(f"\n  {args.cv}-Fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³:")
    cv_scores = cross_validate_model(X_scaled, y, n_splits=args.cv)
    
    print(f"    Accuracy: {cv_scores['accuracy'].mean()*100:.2f}% (Â±{cv_scores['accuracy'].std()*100:.2f}%)")
    print(f"    F1 Score: {cv_scores['f1'].mean()*100:.2f}% (Â±{cv_scores['f1'].std()*100:.2f}%)")
    print(f"    ROC-AUC:  {cv_scores['roc_auc'].mean()*100:.2f}% (Â±{cv_scores['roc_auc'].std()*100:.2f}%)")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    print(f"\n{'='*60}")
    print("ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦")
    print("=" * 60)
    
    feature_importance = dict(zip(used_features, model.feature_importances_))
    sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    
    for i, (feat, imp) in enumerate(sorted_importance, 1):
        bar = 'â–ˆ' * int(imp * 50)
        print(f"  {i:2d}. {feat:25s} {imp*100:5.2f}% {bar}")
    
    # ãƒˆãƒ”ãƒƒã‚¯åˆ¥è©•ä¾¡
    print(f"\n{'='*60}")
    print("ğŸ“Š ãƒˆãƒ”ãƒƒã‚¯åˆ¥è©•ä¾¡")
    print("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å¾©å…ƒ
    test_indices = combined_df.index[len(X_train):]
    
    for topic in target_topics:
        topic_mask = combined_df.loc[test_indices, 'topic'] == topic
        if topic_mask.sum() > 0:
            topic_y_test = y_test[topic_mask.values[:len(y_test)]]
            topic_y_pred = y_pred[topic_mask.values[:len(y_pred)]]
            
            if len(topic_y_test) > 0:
                acc = accuracy_score(topic_y_test, topic_y_pred)
                print(f"  {topic}: Accuracy {acc*100:.1f}% ({len(topic_y_test)}ä»¶)")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print(f"\n{'='*60}")
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜")
    print("=" * 60)
    
    # ãƒ¢ãƒ‡ãƒ«
    model_path = output_dir / "model.pkl"
    joblib.dump(model, model_path)
    print(f"  âœ“ ãƒ¢ãƒ‡ãƒ«: {model_path}")
    
    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
    scaler_path = output_dir / "scaler.pkl"
    joblib.dump(scaler, scaler_path)
    print(f"  âœ“ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: {scaler_path}")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata = {
        'created_at': datetime.now().isoformat(),
        'topics': target_topics,
        'topic_stats': topic_stats,
        'features': used_features,
        'metrics': {
            'test': metrics,
            'cv_accuracy_mean': float(cv_scores['accuracy'].mean()),
            'cv_accuracy_std': float(cv_scores['accuracy'].std()),
            'cv_f1_mean': float(cv_scores['f1'].mean()),
            'cv_f1_std': float(cv_scores['f1'].std()),
            'cv_roc_auc_mean': float(cv_scores['roc_auc'].mean()),
            'cv_roc_auc_std': float(cv_scores['roc_auc'].std()),
        },
        'feature_importance': {k: float(v) for k, v in sorted_importance},
        'model_params': model.get_params(),
    }
    
    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
    print(f"  âœ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_path}")
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
    combined_path = output_dir / "combined_labeled.csv"
    combined_df.to_csv(combined_path, index=False)
    print(f"  âœ“ çµ±åˆãƒ‡ãƒ¼ã‚¿: {combined_path}")
    
    print(f"\n{'='*60}")
    print("âœ… çµ±åˆå­¦ç¿’å®Œäº†ï¼")
    print("=" * 60)
    
    print(f"\nğŸ“Š æœ€çµ‚çµæœ:")
    print(f"  ä½¿ç”¨ãƒˆãƒ”ãƒƒã‚¯: {', '.join(target_topics)}")
    print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(combined_df)}")
    print(f"  ãƒ†ã‚¹ãƒˆ Accuracy: {metrics['accuracy']*100:.2f}%")
    print(f"  ãƒ†ã‚¹ãƒˆ F1 Score: {metrics['f1']*100:.2f}%")
    print(f"  CV Accuracy: {cv_scores['accuracy'].mean()*100:.2f}% (Â±{cv_scores['accuracy'].std()*100:.2f}%)")
    
    print(f"\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"  1. æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ")
    print(f"  2. SHAPåˆ†æã§äºˆæ¸¬ç†ç”±ã‚’å¯è¦–åŒ–")
    print(f"  3. é–¾å€¤èª¿æ•´ã§Precision/Recallã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´")


if __name__ == '__main__':
    main()
