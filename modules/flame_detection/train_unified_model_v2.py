#!/usr/bin/env python3
"""
è¤‡æ•°ãƒˆãƒ”ãƒƒã‚¯çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2 (æ”¹å–„ç‰ˆ)

æ”¹å–„ç‚¹:
1. è¤‡åˆç‰¹å¾´é‡ã®è¿½åŠ ï¼ˆvolume Ã— negative_rate ãªã©ï¼‰
2. é–¾å€¤èª¿æ•´ã§Recallå‘ä¸Š
3. ç‰¹å¾´é‡ã®æ­£è¦åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
4. SMOTE ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

Usage:
    python train_unified_model_v2.py
    python train_unified_model_v2.py --threshold 0.3
    python train_unified_model_v2.py --use-smote
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix,
    precision_recall_curve
)
import xgboost as xgb
import joblib

# åŸºæœ¬ç‰¹å¾´é‡
BASE_FEATURE_COLUMNS = [
    'volume',
    'delta_volume',
    'negative_rate',
    'delta_negative_rate',
    'stance_favor_rate',
    'stance_against_rate',
    'stance_neutral_rate',
]


def discover_topics(base_dir):
    """åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯ã‚’æ¤œå‡º"""
    topics = []
    outputs_dir = Path(base_dir) / "outputs"
    
    if outputs_dir.exists():
        for topic_dir in outputs_dir.iterdir():
            if topic_dir.is_dir():
                labeled_csv = topic_dir / f"{topic_dir.name}_labeled.csv"
                if labeled_csv.exists():
                    topics.append(topic_dir.name)
    
    return sorted(topics)


def load_topic_data(topic_name, base_dir):
    """ãƒˆãƒ”ãƒƒã‚¯ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    csv_path = Path(base_dir) / "outputs" / topic_name / f"{topic_name}_labeled.csv"
    
    if not csv_path.exists():
        print(f"  âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {csv_path}")
        return None
    
    df = pd.read_csv(csv_path)
    df['topic'] = topic_name
    
    return df


def add_composite_features(df):
    """è¤‡åˆç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆæ”¹å–„ãƒã‚¤ãƒ³ãƒˆâ‘ ï¼‰"""
    df = df.copy()
    
    # 1. ç‚ä¸Šã‚¹ã‚³ã‚¢ = volume Ã— negative_rateï¼ˆä¸¡æ–¹é«˜ã„ã¨ç‚ä¸Šã®å¯èƒ½æ€§é«˜ï¼‰
    df['flame_score'] = df['volume'] * df['negative_rate']
    
    # 2. ãƒã‚¬ãƒ†ã‚£ãƒ–æŠ•ç¨¿ã®çµ¶å¯¾æ•°
    df['negative_count'] = df['volume'] * df['negative_rate']
    
    # 3. æ‰¹åˆ¤çš„æŠ•ç¨¿ã®çµ¶å¯¾æ•°ï¼ˆAGAINST Ã— volumeï¼‰
    df['against_count'] = df['volume'] * df['stance_against_rate']
    
    # 4. ãƒã‚¬ãƒ†ã‚£ãƒ–ç‡ã®å¯¾æ•°å¤‰æ›ï¼ˆåˆ†å¸ƒã®æ­£è¦åŒ–ï¼‰
    df['negative_rate_log'] = np.log1p(df['negative_rate'] * 100)
    
    # 5. æŠ•ç¨¿é‡ã®å¯¾æ•°å¤‰æ›ï¼ˆå¤–ã‚Œå€¤ã®å½±éŸ¿è»½æ¸›ï¼‰
    df['volume_log'] = np.log1p(df['volume'])
    
    # 6. æ„Ÿæƒ…æ¥µæ€§ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ç‡ - ãƒã‚¸ãƒ†ã‚£ãƒ–ç‡ã®ä»£ã‚ã‚Šã«ã€stanceä½¿ç”¨ï¼‰
    df['sentiment_polarity'] = df['stance_against_rate'] - df['stance_favor_rate']
    
    # 7. æŠ•ç¨¿é‡ãŒé–¾å€¤ä»¥ä¸Šã‹ã©ã†ã‹ï¼ˆãƒã‚¤ãƒŠãƒªç‰¹å¾´é‡ï¼‰
    df['is_high_volume'] = (df['volume'] >= 50).astype(int)
    
    # 8. ãƒã‚¬ãƒ†ã‚£ãƒ–ç‡ãŒé–¾å€¤ä»¥ä¸Šã‹ã©ã†ã‹
    df['is_high_negative'] = (df['negative_rate'] >= 0.2).astype(int)
    
    # 9. ä¸¡æ–¹é«˜ã„å ´åˆã®ãƒ•ãƒ©ã‚°
    df['is_both_high'] = ((df['volume'] >= 50) & (df['negative_rate'] >= 0.2)).astype(int)
    
    return df


# æ‹¡å¼µç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
EXTENDED_FEATURE_COLUMNS = BASE_FEATURE_COLUMNS + [
    'flame_score',
    'negative_count',
    'against_count',
    'negative_rate_log',
    'volume_log',
    'sentiment_polarity',
    'is_high_volume',
    'is_high_negative',
    'is_both_high',
]


def prepare_features(df, feature_columns):
    """ç‰¹å¾´é‡ã‚’æº–å‚™"""
    available_cols = [col for col in feature_columns if col in df.columns]
    missing_cols = [col for col in feature_columns if col not in df.columns]
    
    if missing_cols:
        print(f"  âš ï¸ æ¬ æç‰¹å¾´é‡: {missing_cols}")
    
    X = df[available_cols].copy()
    X = X.fillna(0)
    X = X.replace([np.inf, -np.inf], 0)
    
    return X, available_cols


def train_model(X_train, y_train, X_test, y_test, class_weight='balanced', params=None):
    """XGBoostãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    
    # ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—
    if class_weight == 'balanced':
        n_neg = (y_train == 0).sum()
        n_pos = (y_train == 1).sum()
        scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0
    else:
        scale_pos_weight = 1.0
    
    print(f"  ã‚¯ãƒ©ã‚¹é‡ã¿ (scale_pos_weight): {scale_pos_weight:.2f}")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    default_params = {
        'n_estimators': 100,
        'max_depth': 3,  # 4â†’3ã«å¤‰æ›´ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
        'learning_rate': 0.05,  # 0.1â†’0.05ã«å¤‰æ›´
        'scale_pos_weight': scale_pos_weight,
        'random_state': 42,
        'use_label_encoder': False,
        'eval_metric': 'logloss',
        'min_child_weight': 3,  # è¿½åŠ ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
        'subsample': 0.8,  # è¿½åŠ 
        'colsample_bytree': 0.8,  # è¿½åŠ 
    }
    
    if params:
        default_params.update(params)
    
    model = xgb.XGBClassifier(**default_params)
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=False
    )
    
    return model


def find_optimal_threshold(model, X_test, y_test, target_recall=0.8):
    """æœ€é©ãªé–¾å€¤ã‚’æ¢ç´¢ï¼ˆæ”¹å–„ãƒã‚¤ãƒ³ãƒˆâ‘¡ï¼‰"""
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, y_pred_proba)
    
    # ç›®æ¨™Recallä»¥ä¸Šã§æœ€å¤§Precisionã®é–¾å€¤ã‚’æ¢ã™
    best_threshold = 0.5
    best_f1 = 0
    
    for i, threshold in enumerate(thresholds):
        if i < len(recall_vals) - 1:
            r = recall_vals[i]
            p = precision_vals[i]
            
            if r >= target_recall and p > 0:
                f1 = 2 * p * r / (p + r)
                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = threshold
    
    return best_threshold


def evaluate_model(model, X_test, y_test, threshold=0.5):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_pred_proba >= threshold).astype(int)
    
    metrics = {
        'threshold': threshold,
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1': f1_score(y_test, y_pred, zero_division=0),
    }
    
    if len(np.unique(y_test)) > 1:
        metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba)
    else:
        metrics['roc_auc'] = None
    
    return metrics, y_pred, y_pred_proba


def cross_validate_model(X, y, n_splits=5, threshold=0.5):
    """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    n_neg = (y == 0).sum()
    n_pos = (y == 1).sum()
    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0
    
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=3,
        learning_rate=0.05,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss',
        min_child_weight=3,
        subsample=0.8,
        colsample_bytree=0.8,
    )
    
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    scores = {
        'accuracy': cross_val_score(model, X, y, cv=cv, scoring='accuracy'),
        'f1': cross_val_score(model, X, y, cv=cv, scoring='f1'),
        'roc_auc': cross_val_score(model, X, y, cv=cv, scoring='roc_auc'),
    }
    
    return scores


def main():
    parser = argparse.ArgumentParser(
        description='è¤‡æ•°ãƒˆãƒ”ãƒƒã‚¯çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2 (æ”¹å–„ç‰ˆ)',
    )
    
    parser.add_argument('--topics', type=str, default=None,
                        help='ä½¿ç”¨ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
    parser.add_argument('--output', type=str, default='outputs/unified_model_v2/',
                        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--test-size', type=float, default=0.2,
                        help='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ')
    parser.add_argument('--cv', type=int, default=5,
                        help='ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ†å‰²æ•°')
    parser.add_argument('--threshold', type=float, default=None,
                        help='åˆ†é¡é–¾å€¤ï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•æ¢ç´¢ï¼‰')
    parser.add_argument('--target-recall', type=float, default=0.8,
                        help='ç›®æ¨™Recallï¼ˆé–¾å€¤è‡ªå‹•æ¢ç´¢æ™‚ï¼‰')
    parser.add_argument('--use-extended-features', action='store_true', default=True,
                        help='æ‹¡å¼µç‰¹å¾´é‡ã‚’ä½¿ç”¨')
    parser.add_argument('--use-smote', action='store_true',
                        help='SMOTEã§ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°')
    
    args = parser.parse_args()
    
    base_dir = Path(__file__).parent
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("ğŸ”¥ è¤‡æ•°ãƒˆãƒ”ãƒƒã‚¯çµ±åˆå­¦ç¿’ v2 (æ”¹å–„ç‰ˆ)")
    print("=" * 60)
    print("\nğŸ“ æ”¹å–„ç‚¹:")
    print("  1. è¤‡åˆç‰¹å¾´é‡ã®è¿½åŠ  (flame_score, negative_countç­‰)")
    print("  2. é–¾å€¤è‡ªå‹•èª¿æ•´ (Recallå‘ä¸Š)")
    print("  3. æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ (éå­¦ç¿’é˜²æ­¢)")
    
    # ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º
    available_topics = discover_topics(base_dir)
    print(f"\nğŸ“‚ åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯: {available_topics}")
    
    if args.topics:
        target_topics = [t.strip() for t in args.topics.split(',')]
    else:
        target_topics = available_topics
    
    print(f"ğŸ“Š ä½¿ç”¨ãƒˆãƒ”ãƒƒã‚¯: {target_topics}")
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    print(f"\n{'='*60}")
    print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»çµ±åˆ")
    print("=" * 60)
    
    all_dfs = []
    topic_stats = {}
    
    for topic in target_topics:
        print(f"\n  ğŸ“ {topic}")
        df = load_topic_data(topic, base_dir)
        
        if df is not None:
            n_total = len(df)
            n_pos = (df['is_controversy'] == 1).sum()
            n_neg = (df['is_controversy'] == 0).sum()
            
            print(f"    ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {n_total}")
            print(f"    ç‚ä¸Š(1): {n_pos} ({n_pos/n_total*100:.1f}%)")
            print(f"    éç‚ä¸Š(0): {n_neg} ({n_neg/n_total*100:.1f}%)")
            
            topic_stats[topic] = {'total': n_total, 'positive': n_pos, 'negative': n_neg}
            all_dfs.append(df)
    
    if not all_dfs:
        print("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    combined_df = pd.concat(all_dfs, ignore_index=True)
    
    # è¤‡åˆç‰¹å¾´é‡ã‚’è¿½åŠ 
    if args.use_extended_features:
        print(f"\nğŸ”§ è¤‡åˆç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...")
        combined_df = add_composite_features(combined_df)
        feature_columns = EXTENDED_FEATURE_COLUMNS
    else:
        feature_columns = BASE_FEATURE_COLUMNS
    
    print(f"\nâœ“ çµ±åˆå®Œäº†: {len(combined_df)}ä»¶")
    total_pos = (combined_df['is_controversy'] == 1).sum()
    total_neg = (combined_df['is_controversy'] == 0).sum()
    print(f"  ç‚ä¸Š(1): {total_pos} ({total_pos/len(combined_df)*100:.1f}%)")
    print(f"  éç‚ä¸Š(0): {total_neg} ({total_neg/len(combined_df)*100:.1f}%)")
    
    # ç‰¹å¾´é‡æº–å‚™
    print(f"\n{'='*60}")
    print("ğŸ”§ ç‰¹å¾´é‡æº–å‚™")
    print("=" * 60)
    
    X, used_features = prepare_features(combined_df, feature_columns)
    y = combined_df['is_controversy'].values
    
    print(f"  ä½¿ç”¨ç‰¹å¾´é‡: {len(used_features)}å€‹")
    for f in used_features:
        print(f"    - {f}")
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # SMOTEã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if args.use_smote:
        try:
            from imblearn.over_sampling import SMOTE
            print(f"\nğŸ“ˆ SMOTEã§ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸­...")
            smote = SMOTE(random_state=42)
            X_scaled, y = smote.fit_resample(X_scaled, y)
            print(f"  ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(y)}ä»¶")
            print(f"  ç‚ä¸Š(1): {(y==1).sum()}, éç‚ä¸Š(0): {(y==0).sum()}")
        except ImportError:
            print("  âš ï¸ imbalanced-learnæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚SMOTEã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    print(f"\n{'='*60}")
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²")
    print("=" * 60)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y,
        test_size=args.test_size,
        random_state=42,
        stratify=y
    )
    
    print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶")
    print(f"    ç‚ä¸Š(1): {(y_train==1).sum()}")
    print(f"    éç‚ä¸Š(0): {(y_train==0).sum()}")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ä»¶")
    print(f"    ç‚ä¸Š(1): {(y_test==1).sum()}")
    print(f"    éç‚ä¸Š(0): {(y_test==0).sum()}")
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    print(f"\n{'='*60}")
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
    print("=" * 60)
    
    model = train_model(X_train, y_train, X_test, y_test, class_weight='balanced')
    
    # é–¾å€¤èª¿æ•´
    print(f"\n{'='*60}")
    print("ğŸ¯ é–¾å€¤èª¿æ•´")
    print("=" * 60)
    
    if args.threshold is not None:
        threshold = args.threshold
        print(f"  æŒ‡å®šé–¾å€¤ã‚’ä½¿ç”¨: {threshold}")
    else:
        threshold = find_optimal_threshold(model, X_test, y_test, target_recall=args.target_recall)
        print(f"  ç›®æ¨™Recall: {args.target_recall*100:.0f}%")
        print(f"  æœ€é©é–¾å€¤: {threshold:.3f}")
    
    # è©•ä¾¡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤0.5ï¼‰
    print(f"\n{'='*60}")
    print("ğŸ“ˆ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
    print("=" * 60)
    
    print(f"\n  ã€é–¾å€¤ 0.5 ã§ã®è©•ä¾¡ã€‘")
    metrics_05, y_pred_05, _ = evaluate_model(model, X_test, y_test, threshold=0.5)
    print(f"    Accuracy:  {metrics_05['accuracy']*100:.2f}%")
    print(f"    Precision: {metrics_05['precision']*100:.2f}%")
    print(f"    Recall:    {metrics_05['recall']*100:.2f}%")
    print(f"    F1 Score:  {metrics_05['f1']*100:.2f}%")
    
    cm_05 = confusion_matrix(y_test, y_pred_05)
    print(f"\n    æ··åŒè¡Œåˆ—:")
    print(f"                äºˆæ¸¬:éç‚ä¸Š  äºˆæ¸¬:ç‚ä¸Š")
    print(f"      å®Ÿéš›:éç‚ä¸Š    {cm_05[0,0]:4d}      {cm_05[0,1]:4d}")
    print(f"      å®Ÿéš›:ç‚ä¸Š      {cm_05[1,0]:4d}      {cm_05[1,1]:4d}")
    
    # è©•ä¾¡ï¼ˆæœ€é©é–¾å€¤ï¼‰
    print(f"\n  ã€é–¾å€¤ {threshold:.3f} ã§ã®è©•ä¾¡ã€‘")
    metrics, y_pred, y_pred_proba = evaluate_model(model, X_test, y_test, threshold=threshold)
    print(f"    Accuracy:  {metrics['accuracy']*100:.2f}%")
    print(f"    Precision: {metrics['precision']*100:.2f}%")
    print(f"    Recall:    {metrics['recall']*100:.2f}%")
    print(f"    F1 Score:  {metrics['f1']*100:.2f}%")
    if metrics['roc_auc']:
        print(f"    ROC-AUC:   {metrics['roc_auc']*100:.2f}%")
    
    cm = confusion_matrix(y_test, y_pred)
    print(f"\n    æ··åŒè¡Œåˆ—:")
    print(f"                äºˆæ¸¬:éç‚ä¸Š  äºˆæ¸¬:ç‚ä¸Š")
    print(f"      å®Ÿéš›:éç‚ä¸Š    {cm[0,0]:4d}      {cm[0,1]:4d}")
    print(f"      å®Ÿéš›:ç‚ä¸Š      {cm[1,0]:4d}      {cm[1,1]:4d}")
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    print(f"\n  {args.cv}-Fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³:")
    cv_scores = cross_validate_model(X_scaled, y, n_splits=args.cv)
    
    print(f"    Accuracy: {cv_scores['accuracy'].mean()*100:.2f}% (Â±{cv_scores['accuracy'].std()*100:.2f}%)")
    print(f"    F1 Score: {cv_scores['f1'].mean()*100:.2f}% (Â±{cv_scores['f1'].std()*100:.2f}%)")
    print(f"    ROC-AUC:  {cv_scores['roc_auc'].mean()*100:.2f}% (Â±{cv_scores['roc_auc'].std()*100:.2f}%)")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    print(f"\n{'='*60}")
    print("ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦")
    print("=" * 60)
    
    feature_importance = dict(zip(used_features, model.feature_importances_))
    sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    
    for i, (feat, imp) in enumerate(sorted_importance, 1):
        bar = 'â–ˆ' * int(imp * 50)
        print(f"  {i:2d}. {feat:25s} {imp*100:5.2f}% {bar}")
    
    # è¦‹é€ƒã—ã‚µãƒ³ãƒ—ãƒ«ã®åˆ†æ
    print(f"\n{'='*60}")
    print("ğŸ” è¦‹é€ƒã—ã‚µãƒ³ãƒ—ãƒ«åˆ†æ")
    print("=" * 60)
    
    missed_mask = (y_test == 1) & (y_pred == 0)
    if missed_mask.sum() > 0:
        print(f"  è¦‹é€ƒã—ä»¶æ•°: {missed_mask.sum()}ä»¶")
        # è¦‹é€ƒã—ãŸã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´ã‚’è¡¨ç¤º
        X_test_df = pd.DataFrame(X_test, columns=used_features)
        missed_samples = X_test_df[missed_mask]
        print(f"  è¦‹é€ƒã—ã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´ï¼ˆæ­£è¦åŒ–å¾Œï¼‰:")
        for col in ['volume', 'negative_rate', 'flame_score']:
            if col in missed_samples.columns:
                print(f"    {col}: {missed_samples[col].values}")
    else:
        print("  âœ“ è¦‹é€ƒã—ãªã—ï¼")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print(f"\n{'='*60}")
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜")
    print("=" * 60)
    
    model_path = output_dir / "model.pkl"
    joblib.dump(model, model_path)
    print(f"  âœ“ ãƒ¢ãƒ‡ãƒ«: {model_path}")
    
    scaler_path = output_dir / "scaler.pkl"
    joblib.dump(scaler, scaler_path)
    print(f"  âœ“ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: {scaler_path}")
    
    metadata = {
        'version': 'v2',
        'created_at': datetime.now().isoformat(),
        'topics': target_topics,
        'topic_stats': topic_stats,
        'features': used_features,
        'threshold': threshold,
        'metrics': {
            'threshold_05': metrics_05,
            'threshold_optimal': metrics,
            'cv_accuracy_mean': float(cv_scores['accuracy'].mean()),
            'cv_accuracy_std': float(cv_scores['accuracy'].std()),
            'cv_f1_mean': float(cv_scores['f1'].mean()),
            'cv_f1_std': float(cv_scores['f1'].std()),
            'cv_roc_auc_mean': float(cv_scores['roc_auc'].mean()),
            'cv_roc_auc_std': float(cv_scores['roc_auc'].std()),
        },
        'feature_importance': {k: float(v) for k, v in sorted_importance},
        'improvements': [
            'è¤‡åˆç‰¹å¾´é‡ (flame_score, negative_countç­‰)',
            'é–¾å€¤è‡ªå‹•èª¿æ•´',
            'æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´',
        ]
    }
    
    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
    print(f"  âœ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_path}")
    
    combined_path = output_dir / "combined_labeled.csv"
    combined_df.to_csv(combined_path, index=False)
    print(f"  âœ“ çµ±åˆãƒ‡ãƒ¼ã‚¿: {combined_path}")
    
    print(f"\n{'='*60}")
    print("âœ… çµ±åˆå­¦ç¿’ v2 å®Œäº†ï¼")
    print("=" * 60)
    
    print(f"\nğŸ“Š æœ€çµ‚çµæœ (é–¾å€¤={threshold:.3f}):")
    print(f"  Recall:    {metrics_05['recall']*100:.2f}% â†’ {metrics['recall']*100:.2f}%")
    print(f"  Precision: {metrics_05['precision']*100:.2f}% â†’ {metrics['precision']*100:.2f}%")
    print(f"  F1 Score:  {metrics_05['f1']*100:.2f}% â†’ {metrics['f1']*100:.2f}%")


if __name__ == '__main__':
    main()
