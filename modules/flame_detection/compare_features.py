#!/usr/bin/env python3
"""
ç‰¹å¾´é‡ã®æ¯”è¼ƒå®Ÿé¨“

ç¾åœ¨ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ vs æ–°ã—ã„ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸã‚»ãƒƒãƒˆã‚’æ¯”è¼ƒ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings('ignore')

# ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
BASE_DIR = Path(__file__).parent.parent.parent


# ===== ç‰¹å¾´é‡å®šç¾© =====

# ç¾åœ¨ã®åŸºæœ¬ç‰¹å¾´é‡
BASE_FEATURES = [
    'volume',
    'delta_volume',
    'negative_rate',
    'delta_negative_rate',
    'stance_favor_rate',
    'stance_against_rate',
    'stance_neutral_rate',
]

# ç¾åœ¨ã®æ‹¡å¼µç‰¹å¾´é‡
CURRENT_EXTENDED = BASE_FEATURES + [
    'flame_score',
    'against_count',
    'negative_rate_log',
    'volume_log',
    'sentiment_polarity',
    'is_high_volume',
    'is_high_negative',
    'is_both_high',
]

# æ–°ã—ãè¿½åŠ ã™ã‚‹ç‰¹å¾´é‡
NEW_FEATURES = [
    'negative_engagement',   # ãƒã‚¬ãƒ†ã‚£ãƒ– Ã— ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ
    'volume_spike_ratio',    # é€šå¸¸æ™‚ã‹ã‚‰ã®ä¹–é›¢ç‡
    'polarization',          # æ„è¦‹ã®äºŒæ¥µåŒ–åº¦
    'negative_momentum',     # ãƒã‚¬ãƒ†ã‚£ãƒ–æ‹¡å¤§ã®å‹¢ã„
]

# é‡è¦åº¦0ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤ã—ãŸæœ€é©åŒ–ç‰ˆ
OPTIMIZED_FEATURES = BASE_FEATURES + [
    'flame_score',
    'against_count',
    'sentiment_polarity',
    # é‡è¦åº¦0ã‚’å‰Šé™¤: negative_rate_log, volume_log, is_high_volume, is_high_negative, is_both_high
]

# æœ€é©åŒ– + æœ‰åŠ¹ãªæ–°ç‰¹å¾´é‡ã‚’è¿½åŠ 
OPTIMIZED_WITH_NEW = OPTIMIZED_FEATURES + [
    'negative_engagement',   # é‡è¦åº¦3ä½
    'negative_momentum',     # é‡è¦åº¦8ä½
    'polarization',          # é‡è¦åº¦13ä½
    # volume_spike_ratioã¯é‡è¦åº¦0ãªã®ã§é™¤å¤–
]


def load_standardized_data(topic_name):
    """æ¨™æº–åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    csv_path = BASE_DIR / "data" / "standardized" / f"{topic_name}.csv"
    if csv_path.exists():
        return pd.read_csv(csv_path)
    return None


def load_labeled_data(topic_name):
    """ãƒ©ãƒ™ãƒ«ä»˜ãæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    csv_path = BASE_DIR / "modules" / "flame_detection" / "outputs" / topic_name / f"{topic_name}_labeled.csv"
    if csv_path.exists():
        df = pd.read_csv(csv_path)
        df['topic'] = topic_name
        return df
    return None


def calculate_engagement_features(topic_name):
    """å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé–¢é€£ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—"""
    # æ¨™æº–åŒ–ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    raw_df = load_standardized_data(topic_name)
    if raw_df is None:
        return None
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å¯¾å¿œï¼‰
    raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'])
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ãŒãªã„å ´åˆã¯JSTã¨ã—ã¦æ‰±ã†
    if raw_df['timestamp'].dt.tz is None:
        raw_df['timestamp'] = raw_df['timestamp'].dt.tz_localize('Asia/Tokyo')
    
    raw_df['hour'] = raw_df['timestamp'].dt.floor('h')
    
    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’è¨ˆç®—
    raw_df['engagement'] = raw_df['like_count'].fillna(0) + raw_df['retweet_count'].fillna(0)
    
    # æ™‚é–“çª“ã”ã¨ã«é›†è¨ˆ
    hourly = raw_df.groupby('hour').agg({
        'engagement': ['sum', 'mean'],
        'like_count': 'sum',
        'retweet_count': 'sum',
    }).reset_index()
    
    hourly.columns = ['timestamp', 'total_engagement', 'avg_engagement', 'total_likes', 'total_retweets']
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã¨ä¸€è‡´ã•ã›ã‚‹ï¼‰
    hourly['timestamp'] = hourly['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S+09:00')
    
    return hourly


def add_current_composite_features(df):
    """ç¾åœ¨ã®è¤‡åˆç‰¹å¾´é‡ã‚’è¿½åŠ """
    df = df.copy()
    
    # 1. ç‚ä¸Šã‚¹ã‚³ã‚¢
    df['flame_score'] = df['volume'] * df['negative_rate']
    
    # 2. æ‰¹åˆ¤çš„æŠ•ç¨¿ã®çµ¶å¯¾æ•°
    df['against_count'] = df['volume'] * df['stance_against_rate']
    
    # 3. ãƒã‚¬ãƒ†ã‚£ãƒ–ç‡ã®å¯¾æ•°å¤‰æ›
    df['negative_rate_log'] = np.log1p(df['negative_rate'] * 100)
    
    # 4. æŠ•ç¨¿é‡ã®å¯¾æ•°å¤‰æ›
    df['volume_log'] = np.log1p(df['volume'])
    
    # 5. æ„Ÿæƒ…æ¥µæ€§
    df['sentiment_polarity'] = df['stance_against_rate'] - df['stance_favor_rate']
    
    # 6. æŠ•ç¨¿é‡ãŒé–¾å€¤ä»¥ä¸Šã‹
    df['is_high_volume'] = (df['volume'] >= 50).astype(int)
    
    # 7. ãƒã‚¬ãƒ†ã‚£ãƒ–ç‡ãŒé–¾å€¤ä»¥ä¸Šã‹
    df['is_high_negative'] = (df['negative_rate'] >= 0.2).astype(int)
    
    # 8. ä¸¡æ–¹é«˜ã„å ´åˆ
    df['is_both_high'] = ((df['volume'] >= 50) & (df['negative_rate'] >= 0.2)).astype(int)
    
    return df


def add_new_features(df):
    """æ–°ã—ã„ç‰¹å¾´é‡ã‚’è¿½åŠ """
    df = df.copy()
    
    # 1. negative_engagement: ãƒã‚¬ãƒ†ã‚£ãƒ– Ã— ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ
    if 'total_engagement' in df.columns:
        df['total_engagement'] = df['total_engagement'].fillna(0)
        df['negative_engagement'] = df['negative_rate'] * df['total_engagement']
    else:
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ volume ã§ä»£ç”¨
        df['negative_engagement'] = df['negative_rate'] * df['volume']
    
    # 2. volume_spike_ratio: é€šå¸¸æ™‚ã‹ã‚‰ã®ä¹–é›¢ç‡
    volume_mean = df['volume'].mean()
    volume_std = df['volume'].std()
    if volume_std > 0:
        df['volume_spike_ratio'] = (df['volume'] - volume_mean) / volume_std
    else:
        df['volume_spike_ratio'] = 0
    
    # 3. polarization: æ„è¦‹ã®äºŒæ¥µåŒ–åº¦ï¼ˆè³›æˆÃ—åå¯¾ï¼‰
    df['polarization'] = df['stance_favor_rate'] * df['stance_against_rate']
    
    # 4. negative_momentum: ãƒã‚¬ãƒ†ã‚£ãƒ–æ‹¡å¤§ã®å‹¢ã„
    # delta_negative_rate Ã— volumeï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ãŒå¢—ãˆã¦ã„ã¦ã€ã‹ã¤é‡ãŒå¤šã„ï¼‰
    df['negative_momentum'] = df['delta_negative_rate'] * df['volume']
    
    return df


def prepare_features(df, feature_columns):
    """ç‰¹å¾´é‡ã‚’æº–å‚™"""
    available_cols = [col for col in feature_columns if col in df.columns]
    missing_cols = [col for col in feature_columns if col not in df.columns]
    
    if missing_cols:
        print(f"    âš ï¸ æ¬ æç‰¹å¾´é‡: {missing_cols}")
    
    X = df[available_cols].copy()
    X = X.fillna(0)
    
    return X, available_cols


def evaluate_model(X, y, feature_name=""):
    """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
    # ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—
    n_pos = y.sum()
    n_neg = len(y) - n_pos
    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1
    
    model = XGBClassifier(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.1,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        verbosity=0,
        use_label_encoder=False,
        eval_metric='logloss'
    )
    
    # 5-fold CV
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    accuracies = []
    f1_scores = []
    roc_aucs = []
    
    for train_idx, test_idx in cv.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        
        accuracies.append(accuracy_score(y_test, y_pred))
        f1_scores.append(f1_score(y_test, y_pred))
        
        if len(np.unique(y_test)) > 1:
            roc_aucs.append(roc_auc_score(y_test, y_proba))
    
    # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆæœ€å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ï¼‰
    importance = dict(zip(X.columns, model.feature_importances_))
    
    return {
        'accuracy': np.mean(accuracies),
        'accuracy_std': np.std(accuracies),
        'f1': np.mean(f1_scores),
        'f1_std': np.std(f1_scores),
        'roc_auc': np.mean(roc_aucs) if roc_aucs else 0,
        'roc_auc_std': np.std(roc_aucs) if roc_aucs else 0,
        'importance': importance,
    }


def main():
    print("=" * 70)
    print("ğŸ”¬ ç‰¹å¾´é‡æ¯”è¼ƒå®Ÿé¨“")
    print("=" * 70)
    
    # ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’å–å¾—
    outputs_dir = BASE_DIR / "modules" / "flame_detection" / "outputs"
    topics = []
    for topic_dir in outputs_dir.iterdir():
        if topic_dir.is_dir():
            labeled_csv = topic_dir / f"{topic_dir.name}_labeled.csv"
            if labeled_csv.exists():
                topics.append(topic_dir.name)
    
    print(f"\nğŸ“Š å¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯: {topics}")
    
    # å…¨ãƒˆãƒ”ãƒƒã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    all_data = []
    engagement_data = {}
    
    for topic in topics:
        df = load_labeled_data(topic)
        if df is not None:
            all_data.append(df)
            # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿
            eng_df = calculate_engagement_features(topic)
            if eng_df is not None:
                engagement_data[topic] = eng_df
            print(f"  âœ… {topic}: {len(df)}ä»¶, ç‚ä¸Šç‡: {df['is_controversy'].mean()*100:.1f}%")
    
    if not all_data:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"\nğŸ“ˆ åˆè¨ˆ: {len(combined_df)}ä»¶, ç‚ä¸Š: {combined_df['is_controversy'].sum()}ä»¶")
    
    # ç¾åœ¨ã®ç‰¹å¾´é‡ã‚’è¿½åŠ 
    combined_df = add_current_composite_features(combined_df)
    
    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
    eng_frames = []
    for topic, eng_df in engagement_data.items():
        eng_df = eng_df.copy()
        eng_df['topic'] = topic
        eng_frames.append(eng_df)
    
    if eng_frames:
        all_engagement = pd.concat(eng_frames, ignore_index=True)
        combined_df = combined_df.merge(
            all_engagement[['timestamp', 'topic', 'total_engagement', 'avg_engagement']],
            on=['timestamp', 'topic'],
            how='left'
        )
    
    # æ–°ã—ã„ç‰¹å¾´é‡ã‚’è¿½åŠ 
    combined_df = add_new_features(combined_df)
    
    # ãƒ©ãƒ™ãƒ«
    y = combined_df['is_controversy']
    
    # 5ã¤ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§æ¯”è¼ƒ
    feature_sets = {
        'â‘  åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ (7å€‹)': BASE_FEATURES,
        'â‘¡ ç¾åœ¨ã®æ‹¡å¼µç‰¹å¾´é‡ (15å€‹)': CURRENT_EXTENDED,
        'â‘¢ å…¨éƒ¨è¿½åŠ  (19å€‹)': CURRENT_EXTENDED + NEW_FEATURES,
        'â‘£ æœ€é©åŒ–ç‰ˆ (10å€‹)': OPTIMIZED_FEATURES,
        'â‘¤ æœ€é©åŒ–+æ–°ç‰¹å¾´é‡ (13å€‹)': OPTIMIZED_WITH_NEW,
    }
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¯”è¼ƒçµæœ")
    print("=" * 70)
    
    results = {}
    
    for name, features in feature_sets.items():
        print(f"\nğŸ”¹ {name}")
        X, used_features = prepare_features(combined_df, features)
        print(f"   ä½¿ç”¨ç‰¹å¾´é‡: {len(used_features)}å€‹")
        
        result = evaluate_model(X, y, name)
        results[name] = result
        
        print(f"   Accuracy: {result['accuracy']*100:.2f}% (Â±{result['accuracy_std']*100:.2f})")
        print(f"   F1 Score: {result['f1']*100:.2f}% (Â±{result['f1_std']*100:.2f})")
        print(f"   ROC-AUC:  {result['roc_auc']*100:.2f}% (Â±{result['roc_auc_std']*100:.2f})")
    
    # æ¯”è¼ƒã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("ğŸ“ˆ æ¯”è¼ƒã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    print(f"\n{'ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ':<30} {'Accuracy':>10} {'F1':>10} {'ROC-AUC':>10}")
    print("-" * 60)
    
    for name, result in results.items():
        short_name = name.split('(')[0].strip()
        print(f"{short_name:<30} {result['accuracy']*100:>9.2f}% {result['f1']*100:>9.2f}% {result['roc_auc']*100:>9.2f}%")
    
    # æ–°ç‰¹å¾´é‡ã®é‡è¦åº¦
    print("\n" + "=" * 70)
    print("ğŸ†• ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆæœ€é©åŒ–+æ–°ç‰¹å¾´é‡ç‰ˆï¼‰")
    print("=" * 70)
    
    new_features_result = results['â‘¤ æœ€é©åŒ–+æ–°ç‰¹å¾´é‡ (13å€‹)']
    importance = new_features_result['importance']
    
    # å…¨ç‰¹å¾´é‡ã‚’é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
    
    print("\nå…¨ç‰¹å¾´é‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    for i, (feat, imp) in enumerate(sorted_importance, 1):
        marker = "ğŸ†•" if feat in NEW_FEATURES else "  "
        print(f"  {i:2}. {marker} {feat:<25} {imp:.4f}")
    
    # æ–°ç‰¹å¾´é‡ã®ã¿
    print("\næ–°ç‰¹å¾´é‡ã®ã¿:")
    for feat in NEW_FEATURES:
        if feat in importance:
            print(f"  - {feat:<25} {importance[feat]:.4f}")
    
    # æ”¹å–„åº¦ã®è¨ˆç®—
    print("\n" + "=" * 70)
    print("ğŸ“Š æ”¹å–„åº¦åˆ†æ")
    print("=" * 70)
    
    baseline = results['â‘¡ ç¾åœ¨ã®æ‹¡å¼µç‰¹å¾´é‡ (15å€‹)']
    optimized = results['â‘£ æœ€é©åŒ–ç‰ˆ (10å€‹)']
    optimized_new = results['â‘¤ æœ€é©åŒ–+æ–°ç‰¹å¾´é‡ (13å€‹)']
    
    print(f"\nã€ç¾åœ¨ vs æœ€é©åŒ–ç‰ˆã€‘ï¼ˆé‡è¦åº¦0ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼‰")
    acc_diff = (optimized['accuracy'] - baseline['accuracy']) * 100
    f1_diff = (optimized['f1'] - baseline['f1']) * 100
    print(f"  Accuracy: {baseline['accuracy']*100:.2f}% â†’ {optimized['accuracy']*100:.2f}% ({'+' if acc_diff >= 0 else ''}{acc_diff:.2f}%)")
    print(f"  F1 Score: {baseline['f1']*100:.2f}% â†’ {optimized['f1']*100:.2f}% ({'+' if f1_diff >= 0 else ''}{f1_diff:.2f}%)")
    print(f"  ç‰¹å¾´é‡æ•°: 15å€‹ â†’ 10å€‹ (5å€‹å‰Šæ¸›)")
    
    print(f"\nã€æœ€é©åŒ–ç‰ˆ vs æœ€é©åŒ–+æ–°ç‰¹å¾´é‡ã€‘")
    acc_diff = (optimized_new['accuracy'] - optimized['accuracy']) * 100
    f1_diff = (optimized_new['f1'] - optimized['f1']) * 100
    print(f"  Accuracy: {optimized['accuracy']*100:.2f}% â†’ {optimized_new['accuracy']*100:.2f}% ({'+' if acc_diff >= 0 else ''}{acc_diff:.2f}%)")
    print(f"  F1 Score: {optimized['f1']*100:.2f}% â†’ {optimized_new['f1']*100:.2f}% ({'+' if f1_diff >= 0 else ''}{f1_diff:.2f}%)")
    
    # æœ€è‰¯ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’åˆ¤å®š
    print("\n" + "=" * 70)
    print("ğŸ† æ¨å¥¨")
    print("=" * 70)
    
    best_name = max(results.items(), key=lambda x: x[1]['f1'])[0]
    best = results[best_name]
    print(f"\næœ€è‰¯ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ: {best_name}")
    print(f"  Accuracy: {best['accuracy']*100:.2f}%")
    print(f"  F1 Score: {best['f1']*100:.2f}%")
    print(f"  ROC-AUC:  {best['roc_auc']*100:.2f}%")


if __name__ == "__main__":
    main()
