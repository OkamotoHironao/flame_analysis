#!/usr/bin/env python3
"""
æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ
XGBoost, Random Forest, LightGBM, CatBoost, SVM, Logistic Regression ã‚’æ¯”è¼ƒ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
import time
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# ãƒ¢ãƒ‡ãƒ«
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# ãƒ‘ã‚¹è¨­å®š
SCRIPT_DIR = Path(__file__).parent
OUTPUTS_DIR = SCRIPT_DIR / "outputs"


def load_all_labeled_data():
    """å…¨ãƒˆãƒ”ãƒƒã‚¯ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    all_data = []
    
    for topic_dir in OUTPUTS_DIR.iterdir():
        if topic_dir.is_dir():
            labeled_csv = topic_dir / f"{topic_dir.name}_labeled.csv"
            if labeled_csv.exists():
                df = pd.read_csv(labeled_csv)
                df['topic'] = topic_dir.name
                all_data.append(df)
                print(f"  âœ“ {topic_dir.name}: {len(df)}ä»¶")
    
    if not all_data:
        raise ValueError("ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    combined = pd.concat(all_data, ignore_index=True)
    print(f"\n  åˆè¨ˆ: {len(combined)}ä»¶")
    
    return combined


def prepare_features(df):
    """ç‰¹å¾´é‡ã‚’æº–å‚™"""
    feature_columns = [
        'volume', 'delta_volume', 'negative_rate', 'delta_negative_rate',
        'stance_favor_rate', 'stance_against_rate', 'stance_neutral_rate',
        'flame_score', 'against_count', 'sentiment_polarity'
    ]
    
    # å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨
    available_features = [col for col in feature_columns if col in df.columns]
    
    # è¤‡åˆç‰¹å¾´é‡ã‚’è¨ˆç®—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
    if 'flame_score' not in df.columns and 'volume' in df.columns and 'negative_rate' in df.columns:
        df['flame_score'] = df['volume'] * df['negative_rate']
        available_features.append('flame_score')
    
    if 'against_count' not in df.columns and 'volume' in df.columns and 'stance_against_rate' in df.columns:
        df['against_count'] = df['volume'] * df['stance_against_rate']
        available_features.append('against_count')
    
    if 'sentiment_polarity' not in df.columns and 'stance_against_rate' in df.columns and 'stance_favor_rate' in df.columns:
        df['sentiment_polarity'] = df['stance_against_rate'] - df['stance_favor_rate']
        available_features.append('sentiment_polarity')
    
    # é‡è¤‡ã‚’é™¤å»
    available_features = list(dict.fromkeys(available_features))
    
    X = df[available_features].fillna(0)
    y = df['is_controversy']
    
    return X, y, available_features


def compare_models(X, y):
    """ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ"""
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©
    models = {
        'XGBoost': XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        ),
        'Random Forest': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        ),
        'LightGBM': LGBMClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42,
            verbose=-1
        ),
        'CatBoost': CatBoostClassifier(
            iterations=100,
            depth=5,
            learning_rate=0.1,
            random_state=42,
            verbose=False
        ),
        'SVM (RBF)': SVC(
            kernel='rbf',
            C=1.0,
            probability=True,
            random_state=42
        ),
        'Logistic Regression': LogisticRegression(
            max_iter=1000,
            random_state=42
        )
    }
    
    # 5-Fold Cross Validation
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    results = []
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆ5-Fold Cross Validationï¼‰")
    print("=" * 70)
    
    for name, model in models.items():
        print(f"\nğŸ”„ {name} ã‚’è©•ä¾¡ä¸­...")
        
        start_time = time.time()
        
        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        acc_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')
        f1_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='f1')
        roc_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='roc_auc')
        
        train_time = time.time() - start_time
        
        result = {
            'model': name,
            'accuracy_mean': acc_scores.mean(),
            'accuracy_std': acc_scores.std(),
            'f1_mean': f1_scores.mean(),
            'f1_std': f1_scores.std(),
            'roc_auc_mean': roc_scores.mean(),
            'roc_auc_std': roc_scores.std(),
            'train_time': train_time
        }
        results.append(result)
        
        print(f"   Accuracy: {acc_scores.mean()*100:.2f}% (Â±{acc_scores.std()*100:.2f}%)")
        print(f"   F1 Score: {f1_scores.mean()*100:.2f}% (Â±{f1_scores.std()*100:.2f}%)")
        print(f"   ROC-AUC:  {roc_scores.mean()*100:.2f}% (Â±{roc_scores.std()*100:.2f}%)")
        print(f"   å­¦ç¿’æ™‚é–“: {train_time:.2f}ç§’")
    
    return pd.DataFrame(results)


def main():
    print("=" * 70)
    print("ğŸ”¬ æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“")
    print("=" * 70)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“‚ ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    df = load_all_labeled_data()
    
    # ç‰¹å¾´é‡æº–å‚™
    print("\nğŸ”§ ç‰¹å¾´é‡æº–å‚™ä¸­...")
    X, y, features = prepare_features(df)
    print(f"   ç‰¹å¾´é‡æ•°: {len(features)}")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(y)} (ç‚ä¸Š: {y.sum()}, éç‚ä¸Š: {len(y) - y.sum()})")
    
    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
    results_df = compare_models(X, y)
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¯”è¼ƒçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    
    # ã‚½ãƒ¼ãƒˆï¼ˆF1ã‚¹ã‚³ã‚¢é™é †ï¼‰
    results_df = results_df.sort_values('f1_mean', ascending=False)
    
    print("\n" + results_df.to_string(index=False))
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
    best_model = results_df.iloc[0]
    print(f"\nğŸ† ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {best_model['model']}")
    print(f"   F1 Score: {best_model['f1_mean']*100:.2f}%")
    print(f"   ROC-AUC: {best_model['roc_auc_mean']*100:.2f}%")
    
    # çµæœä¿å­˜
    output_dir = OUTPUTS_DIR / "model_comparison"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    results_df.to_csv(output_dir / "comparison_results.csv", index=False)
    print(f"\nğŸ’¾ çµæœä¿å­˜: {output_dir / 'comparison_results.csv'}")
    
    return results_df


if __name__ == "__main__":
    main()
