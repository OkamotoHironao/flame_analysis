#!/usr/bin/env python3
"""
ç‚ä¸Šæ¤œçŸ¥åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒ©ãƒ™ãƒ«ä»˜ãç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ is_controversy ã‚’äºˆæ¸¬ã™ã‚‹2å€¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã™ã€‚
"""

import argparse
import pickle
import sys
from pathlib import Path
from typing import Tuple, Dict, Any

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve
)
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier


def load_data(csv_path: str) -> pd.DataFrame:
    """
    ãƒ©ãƒ™ãƒ«ä»˜ãç‰¹å¾´é‡CSVã‚’èª­ã¿è¾¼ã¿
    
    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        pd.DataFrame: èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿
    """
    print(f"ğŸ“– ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {csv_path}")
    
    if not Path(csv_path).exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
    
    df = pd.read_csv(csv_path, comment='#')
    print(f"âœ“ {len(df)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    return df


def prepare_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, list]:
    """
    ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’æº–å‚™
    
    Args:
        df: å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        
    Returns:
        X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        y: ãƒ©ãƒ™ãƒ«ã‚·ãƒªãƒ¼ã‚º
        feature_names: ç‰¹å¾´é‡åã®ãƒªã‚¹ãƒˆ
    """
    print("\nğŸ”§ ç‰¹å¾´é‡æº–å‚™ä¸­...")
    
    # ãƒ©ãƒ™ãƒ«åˆ—ã®ç¢ºèª
    if 'is_controversy' not in df.columns:
        raise ValueError("'is_controversy' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # é™¤å¤–ã™ã‚‹åˆ—ï¼ˆtimestamp, ãƒ©ãƒ™ãƒ«åˆ—ãªã©ï¼‰
    exclude_cols = ['timestamp', 'is_controversy', 'created_at', 'date', 'datetime']
    
    # æ•°å€¤åˆ—ã®ã¿ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # ãƒ©ãƒ™ãƒ«åˆ—ã‚’é™¤å¤–
    feature_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    if not feature_cols:
        raise ValueError("ä½¿ç”¨å¯èƒ½ãªæ•°å€¤ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    print(f"âœ“ ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ï¼ˆ{len(feature_cols)}å€‹ï¼‰:")
    for i, col in enumerate(feature_cols, 1):
        print(f"  {i}. {col}")
    
    X = df[feature_cols].copy()
    y = df['is_controversy'].copy()
    
    # æ¬ æå€¤ã®ç¢ºèª
    missing_count = X.isnull().sum().sum()
    if missing_count > 0:
        print(f"\nâš ï¸  æ¬ æå€¤ã‚’æ¤œå‡º: {missing_count}ä»¶")
        print("  â†’ 0ã§åŸ‹ã‚ã¾ã™")
        X = X.fillna(0)
    
    # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®ç¢ºèª
    print(f"\nğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
    print(f"  éç‚ä¸Š (0): {(y == 0).sum()}ä»¶ ({(y == 0).mean() * 100:.1f}%)")
    print(f"  ç‚ä¸Š (1): {(y == 1).sum()}ä»¶ ({(y == 1).mean() * 100:.1f}%)")
    
    return X, y, feature_cols


def train_model(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    model_type: str = 'xgboost'
) -> Any:
    """
    åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    
    Args:
        X_train: è¨“ç·´ç”¨ç‰¹å¾´é‡
        y_train: è¨“ç·´ç”¨ãƒ©ãƒ™ãƒ«
        model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— ('xgboost' or 'randomforest')
        
    Returns:
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    """
    print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­ï¼ˆ{model_type}ï¼‰...")
    
    # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–: scale_pos_weight ã‚’è¨ˆç®—
    n_negative = (y_train == 0).sum()
    n_positive = (y_train == 1).sum()
    scale_pos_weight = n_negative / n_positive if n_positive > 0 else 1.0
    
    print(f"  ã‚¯ãƒ©ã‚¹é‡ã¿èª¿æ•´: scale_pos_weight={scale_pos_weight:.2f}")
    
    if model_type == 'xgboost':
        model = XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            scale_pos_weight=scale_pos_weight,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
    elif model_type == 'randomforest':
        # RandomForestã®å ´åˆã¯class_weightã§èª¿æ•´
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            class_weight='balanced',
            random_state=42
        )
    else:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
    
    model.fit(X_train, y_train)
    print("âœ“ å­¦ç¿’å®Œäº†")
    
    return model


def evaluate_model(
    model: Any,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    output_dir: Path
) -> Dict[str, float]:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡
    
    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        X_test: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
        y_test: ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
    """
    print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
    
    # äºˆæ¸¬
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1_score': f1_score(y_test, y_pred, zero_division=0),
        'auc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0.0
    }
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ“ˆ è©•ä¾¡çµæœ")
    print("="*60)
    print(f"  Accuracy:  {metrics['accuracy']:.4f}")
    print(f"  Precision: {metrics['precision']:.4f}")
    print(f"  Recall:    {metrics['recall']:.4f}")
    print(f"  F1-Score:  {metrics['f1_score']:.4f}")
    print(f"  AUC:       {metrics['auc']:.4f}")
    print("="*60)
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(y_test, y_pred)
    print("\nğŸ”¢ æ··åŒè¡Œåˆ—:")
    print(cm)
    print(f"\n  True Negative (æ­£ã—ãéç‚ä¸Šã¨äºˆæ¸¬): {cm[0, 0]}")
    print(f"  False Positive (èª¤ã£ã¦ç‚ä¸Šã¨äºˆæ¸¬): {cm[0, 1]}")
    print(f"  False Negative (èª¤ã£ã¦éç‚ä¸Šã¨äºˆæ¸¬): {cm[1, 0]}")
    print(f"  True Positive (æ­£ã—ãç‚ä¸Šã¨äºˆæ¸¬): {cm[1, 1]}")
    
    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nğŸ“‹ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(
        y_test, y_pred,
        target_names=['éç‚ä¸Š', 'ç‚ä¸Š'],
        zero_division=0
    ))
    
    # è©•ä¾¡çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    eval_path = output_dir / 'evaluation.txt'
    with open(eval_path, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("ç‚ä¸Šæ¤œçŸ¥ãƒ¢ãƒ‡ãƒ« è©•ä¾¡çµæœ\n")
        f.write("="*60 + "\n\n")
        
        f.write("ã€è©•ä¾¡æŒ‡æ¨™ã€‘\n")
        f.write(f"  Accuracy:  {metrics['accuracy']:.4f}\n")
        f.write(f"  Precision: {metrics['precision']:.4f}\n")
        f.write(f"  Recall:    {metrics['recall']:.4f}\n")
        f.write(f"  F1-Score:  {metrics['f1_score']:.4f}\n")
        f.write(f"  AUC:       {metrics['auc']:.4f}\n\n")
        
        f.write("ã€æ··åŒè¡Œåˆ—ã€‘\n")
        f.write(f"  True Negative:  {cm[0, 0]}\n")
        f.write(f"  False Positive: {cm[0, 1]}\n")
        f.write(f"  False Negative: {cm[1, 0]}\n")
        f.write(f"  True Positive:  {cm[1, 1]}\n\n")
        
        f.write("ã€åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã€‘\n")
        f.write(classification_report(
            y_test, y_pred,
            target_names=['éç‚ä¸Š', 'ç‚ä¸Š'],
            zero_division=0
        ))
    
    print(f"\nâœ“ è©•ä¾¡çµæœã‚’ä¿å­˜: {eval_path}")
    
    # æ··åŒè¡Œåˆ—ã¨ROCæ›²ç·šã‚’å¯è¦–åŒ–
    visualize_evaluation(y_test, y_pred, y_pred_proba, cm, output_dir)
    
    return metrics


def visualize_evaluation(
    y_test: pd.Series,
    y_pred: np.ndarray,
    y_pred_proba: np.ndarray,
    cm: np.ndarray,
    output_dir: Path
):
    """
    è©•ä¾¡çµæœã‚’å¯è¦–åŒ–
    
    Args:
        y_test: ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«
        y_pred: äºˆæ¸¬ãƒ©ãƒ™ãƒ«
        y_pred_proba: äºˆæ¸¬ç¢ºç‡
        cm: æ··åŒè¡Œåˆ—
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    print("\nğŸ¨ è©•ä¾¡çµæœã‚’å¯è¦–åŒ–ä¸­...")
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # 1. æ··åŒè¡Œåˆ—
    ax1 = axes[0]
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['Non-Controversy', 'Controversy'],
        yticklabels=['Non-Controversy', 'Controversy'],
        ax=ax1
    )
    ax1.set_title('Confusion Matrix', fontsize=14, pad=10)
    ax1.set_ylabel('True Label', fontsize=11)
    ax1.set_xlabel('Predicted Label', fontsize=11)
    
    # 2. ROCæ›²ç·š
    ax2 = axes[1]
    if len(np.unique(y_test)) > 1:
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
        auc_score = roc_auc_score(y_test, y_pred_proba)
        
        ax2.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')
        ax2.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
        ax2.set_xlabel('False Positive Rate', fontsize=11)
        ax2.set_ylabel('True Positive Rate', fontsize=11)
        ax2.set_title('ROC Curve', fontsize=14, pad=10)
        ax2.legend(loc='lower right')
        ax2.grid(True, alpha=0.3)
    else:
        ax2.text(0.5, 0.5, 'AUCè¨ˆç®—ä¸å¯\n(ã‚¯ãƒ©ã‚¹ãŒ1ç¨®é¡ã®ã¿)',
                ha='center', va='center', fontsize=12)
        ax2.set_title('ROC Curve', fontsize=14, pad=10)
    
    plt.tight_layout()
    
    eval_fig_path = output_dir / 'evaluation_metrics.png'
    plt.savefig(eval_fig_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ è©•ä¾¡ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {eval_fig_path}")
    plt.close()


def visualize_feature_importance(
    model: Any,
    feature_names: list,
    output_dir: Path,
    top_n: int = 20
):
    """
    ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¯è¦–åŒ–
    
    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        feature_names: ç‰¹å¾´é‡åã®ãƒªã‚¹ãƒˆ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        top_n: è¡¨ç¤ºã™ã‚‹ä¸Šä½Nå€‹ã®ç‰¹å¾´é‡
    """
    print("\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¯è¦–åŒ–ä¸­...")
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    else:
        print("âš ï¸  ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹å¾´é‡é‡è¦åº¦ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“")
        return
    
    # DataFrameã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    # ä¸Šä½Nå€‹ã‚’è¡¨ç¤º
    print(f"\nğŸ† ç‰¹å¾´é‡é‡è¦åº¦ Top {min(top_n, len(importance_df))}:")
    for i, row in importance_df.head(top_n).iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(10, max(6, len(importance_df.head(top_n)) * 0.4)))
    
    top_features = importance_df.head(top_n)
    
    plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importance', fontsize=12)
    plt.title(f'Feature Importance (Top {len(top_features)})', fontsize=14, pad=15)
    plt.gca().invert_yaxis()  # ä¸Šä½ã‚’ä¸Šã«è¡¨ç¤º
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    
    importance_path = output_dir / 'feature_importance.png'
    plt.savefig(importance_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ ç‰¹å¾´é‡é‡è¦åº¦ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {importance_path}")
    plt.close()
    
    # CSVä¿å­˜
    importance_csv = output_dir / 'feature_importance.csv'
    importance_df.to_csv(importance_csv, index=False, encoding='utf-8')
    print(f"âœ“ ç‰¹å¾´é‡é‡è¦åº¦CSVã‚’ä¿å­˜: {importance_csv}")


def save_model(model: Any, output_dir: Path):
    """
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    
    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    print("\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
    
    model_path = output_dir / 'model.pkl'
    
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {model_path}")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚‚ä¿å­˜
    info_path = output_dir / 'model_info.txt'
    with open(info_path, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æƒ…å ±\n")
        f.write("="*60 + "\n\n")
        f.write(f"ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {type(model).__name__}\n")
        f.write(f"ä¿å­˜ãƒ‘ã‚¹: {model_path}\n\n")
        f.write("ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘\n")
        f.write(str(model.get_params()) + "\n")
    
    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ä¿å­˜: {info_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='ç‚ä¸Šæ¤œçŸ¥åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python train_classifier.py labeled.csv output/
  python train_classifier.py labeled.csv output/ --model randomforest
  python train_classifier.py labeled.csv output/ --test-size 0.3
        """
    )
    
    parser.add_argument(
        'input_csv',
        type=str,
        help='ãƒ©ãƒ™ãƒ«ä»˜ãç‰¹å¾´é‡CSVãƒ•ã‚¡ã‚¤ãƒ«'
    )
    
    parser.add_argument(
        'output_dir',
        type=str,
        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        choices=['xgboost', 'randomforest'],
        default='xgboost',
        help='ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« (default: xgboost)'
    )
    
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ (default: 0.2)'
    )
    
    parser.add_argument(
        '--random-state',
        type=int,
        default=42,
        help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (default: 42)'
    )
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*60)
    print("ğŸ”¥ ç‚ä¸Šæ¤œçŸ¥åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
    print("="*60)
    print(f"  å…¥åŠ›CSV: {args.input_csv}")
    print(f"  å‡ºåŠ›å…ˆ: {args.output_dir}")
    print(f"  ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"  ãƒ†ã‚¹ãƒˆå‰²åˆ: {args.test_size}")
    print("="*60)
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = load_data(args.input_csv)
        
        # 2. ç‰¹å¾´é‡æº–å‚™
        X, y, feature_names = prepare_features(df)
        
        # 3. å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        print(f"\nâœ‚ï¸  ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ä¸­ï¼ˆtrain: {1-args.test_size:.0%}, test: {args.test_size:.0%}ï¼‰...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=args.test_size,
            random_state=args.random_state,
            stratify=y  # ã‚¯ãƒ©ã‚¹æ¯”ç‡ã‚’ä¿æŒ
        )
        print(f"âœ“ è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶")
        print(f"âœ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ä»¶")
        
        # 4. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = train_model(X_train, y_train, model_type=args.model)
        
        # 5. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        metrics = evaluate_model(model, X_test, y_test, output_dir)
        
        # 6. ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
        visualize_feature_importance(model, feature_names, output_dir)
        
        # 7. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        save_model(model, output_dir)
        
        print("\n" + "="*60)
        print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print("="*60)
        print(f"\nğŸ“‚ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - {output_dir / 'model.pkl'}")
        print(f"  - {output_dir / 'evaluation.txt'}")
        print(f"  - {output_dir / 'evaluation_metrics.png'}")
        print(f"  - {output_dir / 'feature_importance.png'}")
        print(f"  - {output_dir / 'feature_importance.csv'}")
        print(f"  - {output_dir / 'model_info.txt'}")
        print()
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
