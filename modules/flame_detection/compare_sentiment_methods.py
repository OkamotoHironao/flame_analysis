#!/usr/bin/env python3
"""
æ„Ÿæƒ…åˆ†ææ‰‹æ³•ã®æ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ¯”è¼ƒå¯¾è±¡:
1. BERT: koheiduck/bert-japanese-finetuned-sentiment
2. è¾æ›¸ãƒ™ãƒ¼ã‚¹: PNè¾æ›¸ã‚’ä½¿ç”¨
3. BERT + è¾æ›¸: ä¸¡æ–¹ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨

Usage:
    python compare_sentiment_methods.py
    python compare_sentiment_methods.py --topics æ¾æœ¬äººå¿—,ä¸‰è‹«,å¯¿å¸ãƒšãƒ­
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report
)
import xgboost as xgb
import joblib

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = Path(__file__).parent.parent.parent
FLAME_DIR = BASE_DIR / "modules" / "flame_detection"
OUTPUTS_DIR = FLAME_DIR / "outputs"


# ========================================
# æ„Ÿæƒ…åˆ†ææ‰‹æ³•ã®æ¯”è¼ƒç”¨ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ
# ========================================
SENTIMENT_FEATURE_SETS = {
    'bert_only': {
        'name': 'BERT ã®ã¿',
        'features': [
            'volume', 'delta_volume',
            'negative_rate', 'delta_negative_rate',
            'stance_favor_rate', 'stance_against_rate', 'stance_neutral_rate',
            'flame_score', 'against_count',
            'negative_rate_log', 'volume_log',
            'sentiment_polarity',
            'is_high_volume', 'is_high_negative', 'is_both_high',
        ],
        'description': 'BERTãƒ™ãƒ¼ã‚¹ã®æ„Ÿæƒ…åˆ†æã®ã¿ã‚’ä½¿ç”¨'
    },
    'dict_only': {
        'name': 'è¾æ›¸ãƒ™ãƒ¼ã‚¹ ã®ã¿',
        'features': [
            'volume', 'delta_volume',
            'dict_negative_rate', 'delta_dict_negative_rate',
            'stance_favor_rate', 'stance_against_rate', 'stance_neutral_rate',
            'dict_flame_score', 'against_count',
            'dict_negative_rate_log', 'volume_log',
            'sentiment_polarity',
            'is_high_volume', 'is_dict_high_negative', 'is_both_dict_high',
        ],
        'description': 'PNè¾æ›¸ãƒ™ãƒ¼ã‚¹ã®æ„Ÿæƒ…åˆ†æã®ã¿ã‚’ä½¿ç”¨'
    },
    'bert_and_dict': {
        'name': 'BERT + è¾æ›¸',
        'features': [
            'volume', 'delta_volume',
            # BERT
            'negative_rate', 'delta_negative_rate',
            'flame_score', 'negative_rate_log',
            'is_high_negative',
            # è¾æ›¸
            'dict_negative_rate', 'delta_dict_negative_rate',
            'dict_flame_score', 'dict_negative_rate_log',
            'is_dict_high_negative',
            # å…±é€š
            'stance_favor_rate', 'stance_against_rate', 'stance_neutral_rate',
            'against_count', 'volume_log',
            'sentiment_polarity',
            'is_high_volume', 'is_both_high', 'is_both_dict_high',
        ],
        'description': 'BERTã¨è¾æ›¸ã®ä¸¡æ–¹ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨'
    },
}

# ========================================
# ç«‹å ´æ¤œå‡ºã®æœ‰ç„¡ã®æ¯”è¼ƒç”¨ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ
# ========================================
STANCE_FEATURE_SETS = {
    'with_stance': {
        'name': 'Stance ã‚ã‚Š',
        'features': [
            'volume', 'delta_volume',
            'negative_rate', 'delta_negative_rate',
            'stance_favor_rate', 'stance_against_rate', 'stance_neutral_rate',
            'flame_score', 'against_count',
            'negative_rate_log', 'volume_log',
            'sentiment_polarity',
            'is_high_volume', 'is_high_negative', 'is_both_high',
        ],
        'description': 'æ„Ÿæƒ…åˆ†æ + ç«‹å ´æ¤œå‡ºï¼ˆè³›æˆ/åå¯¾/ä¸­ç«‹ï¼‰ã‚’ä½¿ç”¨'
    },
    'without_stance': {
        'name': 'Stance ãªã—',
        'features': [
            'volume', 'delta_volume',
            'negative_rate', 'delta_negative_rate',
            'flame_score',
            'negative_rate_log', 'volume_log',
            'is_high_volume', 'is_high_negative', 'is_both_high',
        ],
        'description': 'æ„Ÿæƒ…åˆ†æã®ã¿ã‚’ä½¿ç”¨ï¼ˆç«‹å ´æ¤œå‡ºãªã—ï¼‰'
    },
}

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
FEATURE_SETS = SENTIMENT_FEATURE_SETS


def discover_topics():
    """åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯ã‚’æ¤œå‡º"""
    topics = []
    
    if OUTPUTS_DIR.exists():
        for topic_dir in OUTPUTS_DIR.iterdir():
            if topic_dir.is_dir() and topic_dir.name != 'unified_model_v2':
                labeled_csv = topic_dir / f"{topic_dir.name}_labeled.csv"
                if labeled_csv.exists():
                    topics.append(topic_dir.name)
    
    return sorted(topics)


def load_topic_data(topic_name):
    """ãƒˆãƒ”ãƒƒã‚¯ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    csv_path = OUTPUTS_DIR / topic_name / f"{topic_name}_labeled.csv"
    
    if not csv_path.exists():
        return None
    
    df = pd.read_csv(csv_path)
    df['topic'] = topic_name
    
    return df


def add_dictionary_features(df, topic_name):
    """è¾æ›¸ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆäº‹å‰è¨ˆç®—ãŒå¿…è¦ï¼‰"""
    # è¾æ›¸æ„Ÿæƒ…åˆ†æçµæœã®ãƒ‘ã‚¹ã‚’ç¢ºèª
    dict_sentiment_path = BASE_DIR / "data" / "processed" / f"{topic_name}_dict_sentiment_1h.csv"
    
    if dict_sentiment_path.exists():
        dict_df = pd.read_csv(dict_sentiment_path)
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ãƒãƒ¼ã‚¸
        df = df.merge(
            dict_df[['timestamp', 'dict_negative_rate', 'dict_positive_rate']],
            on='timestamp',
            how='left'
        )
    else:
        # è¾æ›¸ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼å€¤
        df['dict_negative_rate'] = 0.0
        df['dict_positive_rate'] = 0.0
    
    return df


def add_composite_features(df, use_dict=False):
    """è¤‡åˆç‰¹å¾´é‡ã‚’è¿½åŠ """
    df = df.copy()
    
    # BERTé–¢é€£
    if 'negative_rate' in df.columns:
        df['flame_score'] = df['volume'] * df['negative_rate']
        df['negative_rate_log'] = np.log1p(df['negative_rate'] * 100)
        df['is_high_negative'] = (df['negative_rate'] >= 0.2).astype(int)
        df['is_both_high'] = ((df['volume'] >= 50) & (df['negative_rate'] >= 0.2)).astype(int)
    
    # å·®åˆ†
    if 'negative_rate' in df.columns:
        df['delta_negative_rate'] = df['negative_rate'].diff().fillna(0)
    
    # è¾æ›¸é–¢é€£
    if 'dict_negative_rate' in df.columns:
        df['dict_flame_score'] = df['volume'] * df['dict_negative_rate']
        df['dict_negative_rate_log'] = np.log1p(df['dict_negative_rate'] * 100)
        df['is_dict_high_negative'] = (df['dict_negative_rate'] >= 0.2).astype(int)
        df['is_both_dict_high'] = ((df['volume'] >= 50) & (df['dict_negative_rate'] >= 0.2)).astype(int)
        df['delta_dict_negative_rate'] = df['dict_negative_rate'].diff().fillna(0)
    
    # å…±é€š
    df['against_count'] = df['volume'] * df.get('stance_against_rate', 0)
    df['volume_log'] = np.log1p(df['volume'])
    df['sentiment_polarity'] = df.get('stance_against_rate', 0) - df.get('stance_favor_rate', 0)
    df['is_high_volume'] = (df['volume'] >= 50).astype(int)
    df['delta_volume'] = df['volume'].diff().fillna(0)
    
    return df


def prepare_features(df, feature_columns):
    """ç‰¹å¾´é‡ã‚’æº–å‚™"""
    available_cols = [col for col in feature_columns if col in df.columns]
    missing_cols = [col for col in feature_columns if col not in df.columns]
    
    if missing_cols:
        print(f"  âš ï¸ æ¬ æç‰¹å¾´é‡: {missing_cols[:5]}...")
    
    X = df[available_cols].copy()
    X = X.fillna(0)
    X = X.replace([np.inf, -np.inf], 0)
    
    return X, available_cols


def train_and_evaluate(X, y, method_name, n_splits=5):
    """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦è©•ä¾¡"""
    n_neg = (y == 0).sum()
    n_pos = (y == 1).sum()
    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0
    
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=3,
        learning_rate=0.05,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss',
        min_child_weight=3,
        subsample=0.8,
        colsample_bytree=0.8,
    )
    
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    cv_accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    cv_f1 = cross_val_score(model, X, y, cv=cv, scoring='f1')
    cv_roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')
    
    results = {
        'method': method_name,
        'cv_accuracy_mean': cv_accuracy.mean(),
        'cv_accuracy_std': cv_accuracy.std(),
        'cv_f1_mean': cv_f1.mean(),
        'cv_f1_std': cv_f1.std(),
        'cv_roc_auc_mean': cv_roc_auc.mean(),
        'cv_roc_auc_std': cv_roc_auc.std(),
        'n_features': X.shape[1],
        'n_samples': len(y),
    }
    
    return results


def run_comparison(topics=None, output_dir=None):
    """æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    print("=" * 70)
    print("ğŸ”¬ æ„Ÿæƒ…åˆ†ææ‰‹æ³•ã®æ¯”è¼ƒå®Ÿé¨“")
    print("=" * 70)
    
    # ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º
    available_topics = discover_topics()
    print(f"\nğŸ“‚ åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯: {available_topics}")
    
    if topics:
        target_topics = [t.strip() for t in topics.split(',')]
    else:
        target_topics = available_topics
    
    print(f"ğŸ“Š ä½¿ç”¨ãƒˆãƒ”ãƒƒã‚¯: {target_topics}")
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    print(f"\n{'='*60}")
    print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»çµ±åˆ")
    print(f"{'='*60}")
    
    all_data = []
    for topic in target_topics:
        df = load_topic_data(topic)
        if df is not None:
            # è¾æ›¸ç‰¹å¾´é‡ã‚’è¿½åŠ 
            df = add_dictionary_features(df, topic)
            all_data.append(df)
            print(f"  âœ“ {topic}: {len(df)}ä»¶")
    
    if not all_data:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"\nğŸ“Š çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df)}ä»¶")
    
    # è¤‡åˆç‰¹å¾´é‡ã‚’è¿½åŠ 
    combined_df = add_composite_features(combined_df, use_dict=True)
    
    # æ­£è§£ãƒ©ãƒ™ãƒ«
    y = combined_df['is_controversy'].values
    print(f"   ç‚ä¸Š(1): {(y==1).sum()}ä»¶, éç‚ä¸Š(0): {(y==0).sum()}ä»¶")
    
    # æ¯”è¼ƒå®Ÿé¨“
    print(f"\n{'='*60}")
    print("ğŸ”¬ æ¯”è¼ƒå®Ÿé¨“é–‹å§‹")
    print(f"{'='*60}")
    
    results = []
    
    for method_key, method_info in FEATURE_SETS.items():
        print(f"\nğŸ“Œ {method_info['name']}")
        print(f"   {method_info['description']}")
        
        X, used_features = prepare_features(combined_df, method_info['features'])
        print(f"   ä½¿ç”¨ç‰¹å¾´é‡: {len(used_features)}å€‹")
        
        result = train_and_evaluate(X, y, method_info['name'])
        result['method_key'] = method_key
        result['features_used'] = used_features
        results.append(result)
        
        print(f"   Accuracy: {result['cv_accuracy_mean']*100:.1f}% (Â±{result['cv_accuracy_std']*100:.1f})")
        print(f"   F1 Score: {result['cv_f1_mean']*100:.1f}% (Â±{result['cv_f1_std']*100:.1f})")
        print(f"   ROC-AUC:  {result['cv_roc_auc_mean']*100:.1f}% (Â±{result['cv_roc_auc_std']*100:.1f})")
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\n{'='*70}")
    print("ğŸ“Š æ¯”è¼ƒçµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*70}")
    
    print(f"\n{'æ‰‹æ³•':<20} {'Accuracy':<15} {'F1 Score':<15} {'ROC-AUC':<15}")
    print("-" * 65)
    
    for r in results:
        print(f"{r['method']:<20} {r['cv_accuracy_mean']*100:>6.1f}% (Â±{r['cv_accuracy_std']*100:.1f})  "
              f"{r['cv_f1_mean']*100:>6.1f}% (Â±{r['cv_f1_std']*100:.1f})  "
              f"{r['cv_roc_auc_mean']*100:>6.1f}% (Â±{r['cv_roc_auc_std']*100:.1f})")
    
    # æœ€è‰¯æ‰‹æ³•
    best = max(results, key=lambda x: x['cv_f1_mean'])
    print(f"\nğŸ† æœ€è‰¯æ‰‹æ³•: {best['method']} (F1: {best['cv_f1_mean']*100:.1f}%)")
    
    # çµæœã‚’ä¿å­˜
    if output_dir is None:
        output_dir = OUTPUTS_DIR / "comparison_results"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # JSONä¿å­˜
    output_json = output_dir / f"sentiment_comparison_{timestamp}.json"
    with open(output_json, 'w', encoding='utf-8') as f:
        # features_usedã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã«å¤‰æ›
        results_serializable = []
        for r in results:
            r_copy = r.copy()
            r_copy['features_used'] = list(r_copy['features_used'])
            results_serializable.append(r_copy)
        
        json.dump({
            'timestamp': timestamp,
            'comparison_type': 'sentiment',
            'topics': target_topics,
            'n_samples': len(combined_df),
            'results': results_serializable,
            'best_method': best['method'],
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ çµæœä¿å­˜: {output_json}")
    
    # CSVä¿å­˜
    output_csv = output_dir / f"sentiment_comparison_{timestamp}.csv"
    pd.DataFrame(results).to_csv(output_csv, index=False)
    print(f"ğŸ’¾ CSVä¿å­˜: {output_csv}")
    
    return results


def run_stance_comparison(topics=None, output_dir=None):
    """ç«‹å ´æ¤œå‡ºã®æœ‰ç„¡ã®æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    print("=" * 70)
    print("ğŸ”¬ ç«‹å ´æ¤œå‡ºã®æœ‰ç„¡ã®æ¯”è¼ƒå®Ÿé¨“")
    print("=" * 70)
    
    # ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º
    available_topics = discover_topics()
    print(f"\nğŸ“‚ åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯: {available_topics}")
    
    if topics:
        target_topics = [t.strip() for t in topics.split(',')]
    else:
        target_topics = available_topics
    
    print(f"ğŸ“Š ä½¿ç”¨ãƒˆãƒ”ãƒƒã‚¯: {target_topics}")
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    print(f"\n{'='*60}")
    print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»çµ±åˆ")
    print(f"{'='*60}")
    
    all_data = []
    for topic in target_topics:
        df = load_topic_data(topic)
        if df is not None:
            all_data.append(df)
            print(f"  âœ“ {topic}: {len(df)}ä»¶")
    
    if not all_data:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"\nğŸ“Š çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(combined_df)}ä»¶")
    
    # è¤‡åˆç‰¹å¾´é‡ã‚’è¿½åŠ 
    combined_df = add_composite_features(combined_df, use_dict=False)
    
    # æ­£è§£ãƒ©ãƒ™ãƒ«
    y = combined_df['is_controversy'].values
    print(f"   ç‚ä¸Š(1): {(y==1).sum()}ä»¶, éç‚ä¸Š(0): {(y==0).sum()}ä»¶")
    
    # æ¯”è¼ƒå®Ÿé¨“
    print(f"\n{'='*60}")
    print("ğŸ”¬ æ¯”è¼ƒå®Ÿé¨“é–‹å§‹")
    print(f"{'='*60}")
    
    results = []
    
    for method_key, method_info in STANCE_FEATURE_SETS.items():
        print(f"\nğŸ“Œ {method_info['name']}")
        print(f"   {method_info['description']}")
        
        X, used_features = prepare_features(combined_df, method_info['features'])
        print(f"   ä½¿ç”¨ç‰¹å¾´é‡: {len(used_features)}å€‹")
        
        result = train_and_evaluate(X, y, method_info['name'])
        result['method_key'] = method_key
        result['features_used'] = used_features
        results.append(result)
        
        print(f"   Accuracy: {result['cv_accuracy_mean']*100:.1f}% (Â±{result['cv_accuracy_std']*100:.1f})")
        print(f"   F1 Score: {result['cv_f1_mean']*100:.1f}% (Â±{result['cv_f1_std']*100:.1f})")
        print(f"   ROC-AUC:  {result['cv_roc_auc_mean']*100:.1f}% (Â±{result['cv_roc_auc_std']*100:.1f})")
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\n{'='*70}")
    print("ğŸ“Š æ¯”è¼ƒçµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*70}")
    
    print(f"\n{'æ‰‹æ³•':<20} {'Accuracy':<15} {'F1 Score':<15} {'ROC-AUC':<15}")
    print("-" * 65)
    
    for r in results:
        print(f"{r['method']:<20} {r['cv_accuracy_mean']*100:>6.1f}% (Â±{r['cv_accuracy_std']*100:.1f})  "
              f"{r['cv_f1_mean']*100:>6.1f}% (Â±{r['cv_f1_std']*100:.1f})  "
              f"{r['cv_roc_auc_mean']*100:>6.1f}% (Â±{r['cv_roc_auc_std']*100:.1f})")
    
    # å·®åˆ†ã‚’è¨ˆç®—
    with_stance = next((r for r in results if r['method_key'] == 'with_stance'), None)
    without_stance = next((r for r in results if r['method_key'] == 'without_stance'), None)
    
    if with_stance and without_stance:
        diff_f1 = (with_stance['cv_f1_mean'] - without_stance['cv_f1_mean']) * 100
        diff_auc = (with_stance['cv_roc_auc_mean'] - without_stance['cv_roc_auc_mean']) * 100
        print(f"\nğŸ“ˆ ç«‹å ´æ¤œå‡ºã®åŠ¹æœ:")
        print(f"   F1 Score:  {'+' if diff_f1 >= 0 else ''}{diff_f1:.1f}%")
        print(f"   ROC-AUC:   {'+' if diff_auc >= 0 else ''}{diff_auc:.1f}%")
    
    # æœ€è‰¯æ‰‹æ³•
    best = max(results, key=lambda x: x['cv_f1_mean'])
    print(f"\nğŸ† æœ€è‰¯æ‰‹æ³•: {best['method']} (F1: {best['cv_f1_mean']*100:.1f}%)")
    
    # çµæœã‚’ä¿å­˜
    if output_dir is None:
        output_dir = OUTPUTS_DIR / "comparison_results"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # JSONä¿å­˜
    output_json = output_dir / f"stance_comparison_{timestamp}.json"
    with open(output_json, 'w', encoding='utf-8') as f:
        results_serializable = []
        for r in results:
            r_copy = r.copy()
            r_copy['features_used'] = list(r_copy['features_used'])
            results_serializable.append(r_copy)
        
        json.dump({
            'timestamp': timestamp,
            'comparison_type': 'stance',
            'topics': target_topics,
            'n_samples': len(combined_df),
            'results': results_serializable,
            'best_method': best['method'],
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ çµæœä¿å­˜: {output_json}")
    
    # CSVä¿å­˜
    output_csv = output_dir / f"stance_comparison_{timestamp}.csv"
    pd.DataFrame(results).to_csv(output_csv, index=False)
    print(f"ğŸ’¾ CSVä¿å­˜: {output_csv}")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='æ‰‹æ³•æ¯”è¼ƒå®Ÿé¨“')
    parser.add_argument('--topics', type=str, default=None, help='ä½¿ç”¨ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
    parser.add_argument('--output', type=str, default=None, help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--type', type=str, default='sentiment', 
                        choices=['sentiment', 'stance', 'all'],
                        help='æ¯”è¼ƒã‚¿ã‚¤ãƒ—: sentiment(æ„Ÿæƒ…åˆ†æ), stance(ç«‹å ´æ¤œå‡º), all(ä¸¡æ–¹)')
    
    args = parser.parse_args()
    
    if args.type == 'sentiment':
        run_comparison(topics=args.topics, output_dir=args.output)
    elif args.type == 'stance':
        run_stance_comparison(topics=args.topics, output_dir=args.output)
    elif args.type == 'all':
        run_comparison(topics=args.topics, output_dir=args.output)
        print("\n" + "="*70 + "\n")
        run_stance_comparison(topics=args.topics, output_dir=args.output)


if __name__ == '__main__':
    main()
