#!/usr/bin/env python3
"""
Stance Detection å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¥æœ¬èªBERTã‚’ä½¿ã£ãŸç«‹å ´åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
"""

import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from tqdm import tqdm
import pandas as pd
import json

from modules.stance_detection.dataset import StanceDataset, load_dataset_from_csv, LABEL_MAP


# ã‚·ãƒ¼ãƒ‰å›ºå®š
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


# Stance Detection ãƒ¢ãƒ‡ãƒ«
class StanceClassifier(nn.Module):
    """
    BERT + Classification Head
    """
    def __init__(self, model_name, num_labels=3, dropout=0.3):
        super(StanceClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits


def train_epoch(model, dataloader, optimizer, scheduler, device, scaler=None):
    """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    progress_bar = tqdm(dataloader, desc="Training")
    
    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        if scaler is not None:
            with torch.cuda.amp.autocast():
                logits = model(input_ids, attention_mask)
                loss = nn.CrossEntropyLoss()(logits, labels)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            logits = model(input_ids, attention_mask)
            loss = nn.CrossEntropyLoss()(logits, labels)
            loss.backward()
            optimizer.step()
        
        scheduler.step()
        
        total_loss += loss.item()
        _, predicted = torch.max(logits, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{100 * correct / total:.2f}%'
        })
    
    return total_loss / len(dataloader), 100 * correct / total


def evaluate(model, dataloader, device):
    """è©•ä¾¡"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            logits = model(input_ids, attention_mask)
            loss = nn.CrossEntropyLoss()(logits, labels)
            
            total_loss += loss.item()
            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    return total_loss / len(dataloader), 100 * correct / total


def create_sample_data(output_path):
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    samples = [
        ("ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã¯ç´ æ™´ã‚‰ã—ã„ï¼ãŠã™ã™ã‚ã§ã™", "FAVOR"),
        ("æœ€é«˜ã®ä½“é¨“ã§ã—ãŸï¼", "FAVOR"),
        ("ã¨ã¦ã‚‚è‰¯ã„å•†å“ã ã¨æ€ã„ã¾ã™", "FAVOR"),
        ("å®Œç’§ãªå¯¾å¿œã§ã—ãŸ", "FAVOR"),
        ("å•é¡ŒãŒå¤šã™ãã‚‹ã€‚ä½¿ãˆãªã„", "AGAINST"),
        ("æœ€æ‚ªã®ã‚µãƒ¼ãƒ“ã‚¹ã ", "AGAINST"),
        ("äºŒåº¦ã¨åˆ©ç”¨ã—ãŸããªã„", "AGAINST"),
        ("ã²ã©ã„å¯¾å¿œã§ã—ãŸ", "AGAINST"),
        ("æ™®é€šã ã¨æ€ã„ã¾ã™", "NEUTRAL"),
        ("ç‰¹ã«å•é¡Œã¯ãªã„", "NEUTRAL"),
        ("å¯ã‚‚ãªãä¸å¯ã‚‚ãªã", "NEUTRAL"),
        ("ã¾ã‚ã¾ã‚ã§ã™", "NEUTRAL"),
    ] * 10
    
    df = pd.DataFrame(samples, columns=["content", "label"])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df.to_csv(output_path, index=False)
    print(f"âœ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ: {output_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ³•: python train.py <train_csv>")
        print("ä¾‹: python train.py ../../data/processed/stance_train.csv")
        sys.exit(1)
    
    TRAIN_CSV = sys.argv[1]
    MODEL_NAME = "cl-tohoku/bert-base-japanese-v3"
    MAX_LENGTH = 128
    BATCH_SIZE = 16 if len(sys.argv) < 3 else int(sys.argv[2])
    EPOCHS = 5
    LEARNING_RATE = 2e-5
    DROPOUT = 0.3
    MODEL_DIR = "model"
    
    print("=" * 60)
    print("Stance Detection å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    use_amp = torch.cuda.is_available()
    scaler = torch.cuda.amp.GradScaler() if use_amp else None
    if use_amp:
        print("âš¡ æ··åˆç²¾åº¦å­¦ç¿’ã‚’æœ‰åŠ¹åŒ–")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
    print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = StanceClassifier(MODEL_NAME, num_labels=3, dropout=DROPOUT)
    model.to(device)
    print("âœ“ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {TRAIN_CSV}")
    if not Path(TRAIN_CSV).exists():
        print("âš ï¸  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™...")
        create_sample_data(TRAIN_CSV)
    
    dataset = load_dataset_from_csv(TRAIN_CSV, tokenizer, MAX_LENGTH)
    
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(dataset)}ä»¶")
    print(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_size}ä»¶")
    print(f"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {val_size}ä»¶")
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    total_steps = len(train_loader) * EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),
        num_training_steps=total_steps
    )
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print(f"\nğŸš€ å­¦ç¿’é–‹å§‹ï¼ˆ{EPOCHS}ã‚¨ãƒãƒƒã‚¯ï¼‰")
    best_val_acc = 0
    
    for epoch in range(EPOCHS):
        print(f"\nã€ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{EPOCHS}ã€‘")
        
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device, scaler)
        print(f"è¨“ç·´ - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%")
        
        val_loss, val_acc = evaluate(model, val_loader, device)
        print(f"æ¤œè¨¼ - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%")
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            os.makedirs(MODEL_DIR, exist_ok=True)
            torch.save(model.state_dict(), f"{MODEL_DIR}/best_model.pth")
            tokenizer.save_pretrained(MODEL_DIR)
            
            config = {
                "model_name": MODEL_NAME,
                "num_labels": 3,
                "max_length": MAX_LENGTH,
                "dropout": DROPOUT
            }
            with open(f"{MODEL_DIR}/config.json", 'w') as f:
                json.dump(config, f, indent=2)
            
            print(f"âœ¨ æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆAcc: {best_val_acc:.2f}%ï¼‰")
    
    print("\n" + "=" * 60)
    print(f"âœ… å­¦ç¿’å®Œäº†ï¼ ãƒ™ã‚¹ãƒˆç²¾åº¦: {best_val_acc:.2f}%")
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {MODEL_DIR}/")
    print("=" * 60)


if __name__ == "__main__":
    main()
