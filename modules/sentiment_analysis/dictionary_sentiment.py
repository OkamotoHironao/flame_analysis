#!/usr/bin/env python3
"""
è¾žæ›¸ãƒ™ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æžãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

PNè¾žæ›¸ã‚’ä½¿ç”¨ã—ãŸå¾“æ¥åž‹ã®æ„Ÿæƒ…åˆ†æž
BERTã¨ã®æ¯”è¼ƒç”¨
"""

import pandas as pd
import numpy as np
from pathlib import Path
import MeCab
import re
from tqdm import tqdm


class DictionarySentiment:
    """è¾žæ›¸ãƒ™ãƒ¼ã‚¹ã®æ„Ÿæƒ…åˆ†æžã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, dictionary_path=None):
        """
        Args:
            dictionary_path: PNè¾žæ›¸ã®ãƒ‘ã‚¹ï¼ˆCSVå½¢å¼: word,polarityï¼‰
        """
        if dictionary_path is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹
            base_dir = Path(__file__).parent.parent.parent
            dictionary_path = base_dir / "data" / "dictionary" / "pn_ja.csv"
        
        self.dictionary_path = Path(dictionary_path)
        self.pn_dict = {}
        self._load_dictionary()
        
        # MeCabåˆæœŸåŒ–
        self.mecab = MeCab.Tagger("-Owakati")
    
    def _load_dictionary(self):
        """PNè¾žæ›¸ã‚’èª­ã¿è¾¼ã‚€"""
        if not self.dictionary_path.exists():
            print(f"âš ï¸ è¾žæ›¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.dictionary_path}")
            return
        
        df = pd.read_csv(self.dictionary_path)
        for _, row in df.iterrows():
            word = str(row['word']).strip()
            polarity = row['polarity']
            self.pn_dict[word] = polarity
        
        print(f"âœ“ PNè¾žæ›¸èª­ã¿è¾¼ã¿å®Œäº†: {len(self.pn_dict)}èªž")
    
    def tokenize(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æž"""
        if not isinstance(text, str):
            return []
        
        # å‰å‡¦ç†
        text = re.sub(r'https?://\S+', '', text)  # URLé™¤åŽ»
        text = re.sub(r'@\w+', '', text)  # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³é™¤åŽ»
        text = re.sub(r'#\w+', '', text)  # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°é™¤åŽ»
        
        try:
            result = self.mecab.parse(text)
            tokens = result.strip().split()
            return tokens
        except:
            return []
    
    def analyze_text(self, text):
        """
        1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„Ÿæƒ…åˆ†æž
        
        Returns:
            dict: {
                'positive_score': float,  # ãƒã‚¸ãƒ†ã‚£ãƒ–èªžæ•°
                'negative_score': float,  # ãƒã‚¬ãƒ†ã‚£ãƒ–èªžæ•°
                'polarity': float,  # æ¥µæ€§ã‚¹ã‚³ã‚¢ (-1 to 1)
                'sentiment': str,  # 'positive', 'negative', 'neutral'
                'word_count': int,
            }
        """
        tokens = self.tokenize(text)
        
        positive_count = 0
        negative_count = 0
        
        for token in tokens:
            if token in self.pn_dict:
                polarity = self.pn_dict[token]
                if polarity > 0:
                    positive_count += polarity
                elif polarity < 0:
                    negative_count += abs(polarity)
        
        total = positive_count + negative_count
        
        if total == 0:
            polarity = 0.0
            sentiment = 'neutral'
        else:
            polarity = (positive_count - negative_count) / total
            if polarity > 0.1:
                sentiment = 'positive'
            elif polarity < -0.1:
                sentiment = 'negative'
            else:
                sentiment = 'neutral'
        
        return {
            'positive_score': positive_count,
            'negative_score': negative_count,
            'polarity': polarity,
            'sentiment': sentiment,
            'word_count': len(tokens),
        }
    
    def analyze_dataframe(self, df, text_column='text', batch_size=1000):
        """
        DataFrameã«å¯¾ã—ã¦æ„Ÿæƒ…åˆ†æžã‚’å®Ÿè¡Œ
        
        Returns:
            pd.DataFrame: æ„Ÿæƒ…åˆ†æžçµæžœãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
        """
        results = []
        
        print(f"ðŸ“– è¾žæ›¸ãƒ™ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æž: {len(df)}ä»¶")
        
        for i, row in tqdm(df.iterrows(), total=len(df), desc="è¾žæ›¸åˆ†æž"):
            text = row.get(text_column, '')
            result = self.analyze_text(text)
            results.append(result)
        
        result_df = pd.DataFrame(results)
        
        # å…ƒã®DataFrameã¨çµåˆ
        output_df = df.copy()
        output_df['dict_positive_score'] = result_df['positive_score']
        output_df['dict_negative_score'] = result_df['negative_score']
        output_df['dict_polarity'] = result_df['polarity']
        output_df['dict_sentiment'] = result_df['sentiment']
        
        return output_df


def analyze_csv(input_path, output_path=None, text_column='text'):
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦è¾žæ›¸ãƒ™ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æžã‚’å®Ÿè¡Œ
    """
    print(f"\n{'='*60}")
    print("ðŸ“– è¾žæ›¸ãƒ™ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æž")
    print(f"{'='*60}")
    
    df = pd.read_csv(input_path)
    print(f"âœ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {input_path} ({len(df)}ä»¶)")
    
    analyzer = DictionarySentiment()
    result_df = analyzer.analyze_dataframe(df, text_column=text_column)
    
    # çµ±è¨ˆè¡¨ç¤º
    sentiments = result_df['dict_sentiment'].value_counts(normalize=True) * 100
    print(f"\nðŸ“Š æ„Ÿæƒ…åˆ†å¸ƒ:")
    for s, pct in sentiments.items():
        print(f"  {s}: {pct:.1f}%")
    
    if output_path:
        result_df.to_csv(output_path, index=False)
        print(f"\nðŸ’¾ å‡ºåŠ›: {output_path}")
    
    return result_df


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='è¾žæ›¸ãƒ™ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æž')
    parser.add_argument('input', help='å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('-o', '--output', help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('-t', '--text-column', default='text', help='ãƒ†ã‚­ã‚¹ãƒˆåˆ—å')
    
    args = parser.parse_args()
    
    output = args.output or args.input.replace('.csv', '_dict_sentiment.csv')
    analyze_csv(args.input, output, args.text_column)
