#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
6ãƒ¢ãƒ‡ãƒ« Ã— 5ãƒˆãƒ”ãƒƒã‚¯ Ã— 2ç‰¹å¾´é‡ã‚»ãƒƒãƒˆï¼ˆ10ç‰¹å¾´ vs 16ç‰¹å¾´ï¼‰ã®å®Œå…¨æ¯”è¼ƒ

ç›®çš„: æœ€é©ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ã‚’æ±ºå®š
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')
import time

print("=" * 100)
print("åŒ…æ‹¬çš„è©•ä¾¡å®Ÿé¨“: 6ãƒ¢ãƒ‡ãƒ« Ã— 5ãƒˆãƒ”ãƒƒã‚¯ Ã— 2ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ")
print("=" * 100)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('outputs/unified_model_v2/combined_labeled.csv')

if 'is_controversy' in df.columns:
    df['label'] = df['is_controversy']
else:
    df['label'] = df['is_flame']

print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±")
print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(df)}ä»¶")
print(f"  ç‚ä¸Š: {(df['label']==1).sum()}ä»¶, éç‚ä¸Š: {(df['label']==0).sum()}ä»¶")
print(f"  ãƒˆãƒ”ãƒƒã‚¯æ•°: {df['topic'].nunique()}å€‹")

# ç‰¹å¾´é‡å®šç¾©
FEATURES_10 = [
    'volume', 'negative_rate', 'stance_against_rate',
    'stance_favor_rate', 'stance_neutral_rate',
    'delta_volume', 'delta_volume_rate',
    'flame_score', 'against_count', 'sentiment_polarity'
]

FEATURES_16 = FEATURES_10 + [
    'delta_negative_rate', 'delta_against_rate',
    'sentiment_avg_score',
    'stance_against_mean', 'stance_favor_mean', 'stance_neutral_mean'
]

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
def get_models():
    """6ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™ (model, use_scaling)"""
    return {
        'CatBoost': (CatBoostClassifier(
            iterations=100, depth=5, learning_rate=0.1,
            random_state=42, verbose=0
        ), False),
        
        'XGBoost': (XGBClassifier(
            n_estimators=100, max_depth=5, learning_rate=0.1,
            random_state=42, use_label_encoder=False, eval_metric='logloss'
        ), False),
        
        'LightGBM': (LGBMClassifier(
            n_estimators=100, learning_rate=0.05, max_depth=5,
            num_leaves=31, random_state=42, verbose=-1
        ), False),
        
        'Random Forest': (RandomForestClassifier(
            n_estimators=100, max_depth=10, random_state=42
        ), False),
        
        'SVM (RBF)': (SVC(
            kernel='rbf', C=10, gamma='scale', random_state=42
        ), True),
        
        'Logistic Regression': (LogisticRegression(
            C=1.0, max_iter=1000, random_state=42
        ), True)
    }

# =================================================================
# å®Ÿé¨“1: æ¨™æº–è©•ä¾¡ï¼ˆå…¨ãƒˆãƒ”ãƒƒã‚¯æ··åœ¨ï¼‰
# =================================================================
print("\n" + "=" * 100)
print("å®Ÿé¨“1: æ¨™æº–è©•ä¾¡ï¼ˆå…¨ãƒˆãƒ”ãƒƒã‚¯æ··åœ¨ã€80/20åˆ†å‰²ï¼‰")
print("=" * 100)

results_standard = {}

for feature_name, features in [("10ç‰¹å¾´é‡", FEATURES_10), ("16ç‰¹å¾´é‡", FEATURES_16)]:
    print(f"\n{'='*50}")
    print(f"ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ: {feature_name} ({len(features)}å€‹)")
    print(f"{'='*50}")
    
    results_standard[feature_name] = {}
    
    X = df[features].fillna(0).replace([np.inf, -np.inf], 0)
    y = df['label']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    for model_name, (model, use_scaling) in get_models().items():
        X_tr = X_train_scaled if use_scaling else X_train
        X_te = X_test_scaled if use_scaling else X_test
        
        # è¨“ç·´
        start_time = time.time()
        model.fit(X_tr, y_train)
        train_time = time.time() - start_time
        
        # äºˆæ¸¬
        y_pred = model.predict(X_te)
        
        # è©•ä¾¡
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, zero_division=0)
        rec = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        # CVè©•ä¾¡
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='f1')
        
        results_standard[feature_name][model_name] = {
            'accuracy': acc,
            'precision': prec,
            'recall': rec,
            'f1': f1,
            'cv_f1_mean': cv_scores.mean(),
            'cv_f1_std': cv_scores.std(),
            'train_time': train_time
        }
        
        print(f"  {model_name:20s} | F1:{f1*100:6.2f}% | Acc:{acc*100:6.2f}% | "
              f"Prec:{prec*100:6.2f}% | Rec:{rec*100:6.2f}% | "
              f"CV:{cv_scores.mean()*100:6.2f}Â±{cv_scores.std()*100:4.2f}% | "
              f"Time:{train_time:5.2f}s")

# =================================================================
# å®Ÿé¨“2: ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯è©•ä¾¡ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ« Ã— å…¨ãƒˆãƒ”ãƒƒã‚¯ï¼‰
# =================================================================
print("\n" + "=" * 100)
print("å®Ÿé¨“2: ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯è©•ä¾¡ï¼ˆLeave-One-Topic-Outï¼‰")
print("=" * 100)

def cross_topic_evaluation_all_models(df, features):
    """å…¨ãƒ¢ãƒ‡ãƒ«ã§ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯è©•ä¾¡"""
    topics = df['topic'].unique()
    results = []
    
    for test_topic in topics:
        train_df = df[df['topic'] != test_topic]
        test_df = df[df['topic'] == test_topic]
        
        if len(test_df) < 5:
            continue
        
        X_train = train_df[features].fillna(0).replace([np.inf, -np.inf], 0)
        y_train = train_df['label']
        X_test = test_df[features].fillna(0).replace([np.inf, -np.inf], 0)
        y_test = test_df['label']
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        topic_results = {'topic': test_topic, 'n_test': len(test_df)}
        
        for model_name, (model, use_scaling) in get_models().items():
            X_tr = X_train_scaled if use_scaling else X_train
            X_te = X_test_scaled if use_scaling else X_test
            
            model.fit(X_tr, y_train)
            y_pred = model.predict(X_te)
            
            f1 = f1_score(y_test, y_pred, zero_division=0)
            acc = accuracy_score(y_test, y_pred)
            
            topic_results[f'{model_name}_f1'] = f1
            topic_results[f'{model_name}_acc'] = acc
        
        results.append(topic_results)
    
    return pd.DataFrame(results)

results_cross_topic = {}

for feature_name, features in [("10ç‰¹å¾´é‡", FEATURES_10), ("16ç‰¹å¾´é‡", FEATURES_16)]:
    print(f"\n{'='*50}")
    print(f"ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ: {feature_name}")
    print(f"{'='*50}")
    
    results_df = cross_topic_evaluation_all_models(df, features)
    results_cross_topic[feature_name] = results_df
    
    # ãƒˆãƒ”ãƒƒã‚¯åˆ¥çµæœè¡¨ç¤º
    print("\nãƒˆãƒ”ãƒƒã‚¯åˆ¥ F1ã‚¹ã‚³ã‚¢:")
    print("â”€" * 100)
    header = f"{'ãƒˆãƒ”ãƒƒã‚¯':12s} | {'Test':4s}"
    for model_name in get_models().keys():
        header += f" | {model_name:12s}"
    print(header)
    print("â”€" * 100)
    
    for _, row in results_df.iterrows():
        line = f"{row['topic']:12s} | {int(row['n_test']):4d}"
        for model_name in get_models().keys():
            f1_val = row[f'{model_name}_f1'] * 100
            line += f" | {f1_val:11.2f}%"
        print(line)
    
    # å„ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡F1
    print("\nå„ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡F1ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯ï¼‰:")
    print("â”€" * 50)
    for model_name in get_models().keys():
        avg_f1 = results_df[f'{model_name}_f1'].mean() * 100
        std_f1 = results_df[f'{model_name}_f1'].std() * 100
        print(f"  {model_name:20s}: {avg_f1:6.2f}% Â± {std_f1:5.2f}%")

# =================================================================
# ç·åˆæ¯”è¼ƒã‚µãƒãƒªãƒ¼
# =================================================================
print("\n" + "=" * 100)
print("ğŸ“Š ç·åˆæ¯”è¼ƒã‚µãƒãƒªãƒ¼")
print("=" * 100)

# æ¨™æº–è©•ä¾¡ã®æ¯”è¼ƒ
print("\nã€æ¨™æº–è©•ä¾¡ï¼ˆå…¨ãƒˆãƒ”ãƒƒã‚¯æ··åœ¨ï¼‰ã€‘")
print("\n10ç‰¹å¾´é‡:")
print("â”€" * 100)
print(f"{'ãƒ¢ãƒ‡ãƒ«':20s} | {'F1':7s} | {'Accuracy':8s} | {'Precision':9s} | {'Recall':7s} | {'CV F1':12s}")
print("â”€" * 100)
for model_name in get_models().keys():
    r = results_standard["10ç‰¹å¾´é‡"][model_name]
    print(f"{model_name:20s} | {r['f1']*100:6.2f}% | {r['accuracy']*100:7.2f}% | "
          f"{r['precision']*100:8.2f}% | {r['recall']*100:6.2f}% | "
          f"{r['cv_f1_mean']*100:5.2f}Â±{r['cv_f1_std']*100:4.2f}%")

print("\n16ç‰¹å¾´é‡:")
print("â”€" * 100)
print(f"{'ãƒ¢ãƒ‡ãƒ«':20s} | {'F1':7s} | {'Accuracy':8s} | {'Precision':9s} | {'Recall':7s} | {'CV F1':12s}")
print("â”€" * 100)
for model_name in get_models().keys():
    r = results_standard["16ç‰¹å¾´é‡"][model_name]
    print(f"{model_name:20s} | {r['f1']*100:6.2f}% | {r['accuracy']*100:7.2f}% | "
          f"{r['precision']*100:8.2f}% | {r['recall']*100:6.2f}% | "
          f"{r['cv_f1_mean']*100:5.2f}Â±{r['cv_f1_std']*100:4.2f}%")

# ãƒ¢ãƒ‡ãƒ«åˆ¥ã®æ”¹å–„åº¦
print("\n" + "=" * 100)
print("ğŸ“ˆ ãƒ¢ãƒ‡ãƒ«åˆ¥æ”¹å–„åº¦ï¼ˆ10ç‰¹å¾´é‡ â†’ 16ç‰¹å¾´é‡ï¼‰")
print("=" * 100)
print(f"{'ãƒ¢ãƒ‡ãƒ«':20s} | {'F1å¤‰åŒ–':10s} | {'Accuracyå¤‰åŒ–':12s} | {'CV F1å¤‰åŒ–':12s} | {'ã‚¯ãƒ­ã‚¹F1å¤‰åŒ–':14s}")
print("â”€" * 100)

for model_name in get_models().keys():
    r10_std = results_standard["10ç‰¹å¾´é‡"][model_name]
    r16_std = results_standard["16ç‰¹å¾´é‡"][model_name]
    
    f1_diff = (r16_std['f1'] - r10_std['f1']) * 100
    acc_diff = (r16_std['accuracy'] - r10_std['accuracy']) * 100
    cv_diff = (r16_std['cv_f1_mean'] - r10_std['cv_f1_mean']) * 100
    
    # ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯å·®åˆ†
    cross10 = results_cross_topic["10ç‰¹å¾´é‡"][f'{model_name}_f1'].mean()
    cross16 = results_cross_topic["16ç‰¹å¾´é‡"][f'{model_name}_f1'].mean()
    cross_diff = (cross16 - cross10) * 100
    
    print(f"{model_name:20s} | {f1_diff:+9.2f}% | {acc_diff:+11.2f}% | "
          f"{cv_diff:+11.2f}% | {cross_diff:+13.2f}%")

# =================================================================
# æœ€çµ‚æ¨å¥¨
# =================================================================
print("\n" + "=" * 100)
print("ğŸ’¡ æœ€çµ‚æ¨å¥¨")
print("=" * 100)

# å„ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§ã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«
print("\nğŸ† å„ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§ã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«:")

for feature_name in ["10ç‰¹å¾´é‡", "16ç‰¹å¾´é‡"]:
    print(f"\nã€{feature_name}ã€‘")
    
    # æ¨™æº–è©•ä¾¡ã§ã®æœ€è‰¯
    best_std = max(results_standard[feature_name].items(), 
                   key=lambda x: x[1]['f1'])
    print(f"  æ¨™æº–è©•ä¾¡:      {best_std[0]:20s} F1:{best_std[1]['f1']*100:.2f}%")
    
    # ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯ã§ã®æœ€è‰¯
    cross_results = results_cross_topic[feature_name]
    model_avg_f1 = {}
    for model_name in get_models().keys():
        model_avg_f1[model_name] = cross_results[f'{model_name}_f1'].mean()
    best_cross = max(model_avg_f1.items(), key=lambda x: x[1])
    print(f"  ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯: {best_cross[0]:20s} F1:{best_cross[1]*100:.2f}%")

# ç·åˆæ¨å¥¨
print("\nğŸ¯ ç·åˆæ¨å¥¨:")

# 16ç‰¹å¾´é‡ç‰ˆã§æ”¹å–„ã—ãŸãƒ¢ãƒ‡ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
improved_count = 0
for model_name in get_models().keys():
    cross10 = results_cross_topic["10ç‰¹å¾´é‡"][f'{model_name}_f1'].mean()
    cross16 = results_cross_topic["16ç‰¹å¾´é‡"][f'{model_name}_f1'].mean()
    if cross16 > cross10:
        improved_count += 1

if improved_count >= 4:  # 6ãƒ¢ãƒ‡ãƒ«ä¸­4ä»¥ä¸Šæ”¹å–„
    print("  âœ… 16ç‰¹å¾´é‡ç‰ˆã‚’æ¨å¥¨")
    print(f"     - {improved_count}/6ãƒ¢ãƒ‡ãƒ«ã§ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯æ€§èƒ½ãŒå‘ä¸Š")
    print("     - sentiment_avg_score, stance_meanç³»ãŒæ±åŒ–ã«å¯„ä¸")
    
    # æœ€è‰¯ã®çµ„ã¿åˆã‚ã›
    best_combo = None
    best_f1 = 0
    for model_name in get_models().keys():
        std_f1 = results_standard["16ç‰¹å¾´é‡"][model_name]['f1']
        cross_f1 = results_cross_topic["16ç‰¹å¾´é‡"][f'{model_name}_f1'].mean()
        avg_f1 = (std_f1 + cross_f1) / 2
        if avg_f1 > best_f1:
            best_f1 = avg_f1
            best_combo = (model_name, std_f1, cross_f1)
    
    print(f"\n  ğŸŒŸ æœ€è‰¯ã®çµ„ã¿åˆã‚ã›: {best_combo[0]} + 16ç‰¹å¾´é‡")
    print(f"     æ¨™æº–è©•ä¾¡F1:      {best_combo[1]*100:.2f}%")
    print(f"     ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯F1: {best_combo[2]*100:.2f}%")
    print(f"     å¹³å‡F1:          {best_f1*100:.2f}%")
    
else:
    print("  âš ï¸  10ç‰¹å¾´é‡ç‰ˆã‚’æ¨å¥¨")
    print(f"     - {6-improved_count}/6ãƒ¢ãƒ‡ãƒ«ã§ã‚¯ãƒ­ã‚¹ãƒˆãƒ”ãƒƒã‚¯æ€§èƒ½ãŒä½ä¸‹")
    print("     - è¿½åŠ ç‰¹å¾´é‡ã®åŠ¹æœãŒé™å®šçš„")

print("\n" + "=" * 100)
