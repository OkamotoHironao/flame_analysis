#!/usr/bin/env python3
"""
ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿(ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç„¡ã—)ã‚’ä½¿ç”¨ã—ã¦æ¤œè¨¼
"""

import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import json

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'IPAGothic', 'Noto Sans CJK JP']
plt.rcParams['axes.unicode_minus'] = False


def train_and_evaluate(X_train, y_train, X_test, y_test, feature_name, feature_cols):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦è©•ä¾¡"""
    print(f"\n{'='*60}")
    print(f"ğŸ”§ {feature_name}")
    print(f"{'='*60}")
    print(f"  ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
    print(f"  è¨“ç·´: {len(X_train)}ä»¶, ãƒ†ã‚¹ãƒˆ: {len(X_test)}ä»¶")
    
    # LightGBMãƒ¢ãƒ‡ãƒ«
    model = lgb.LGBMClassifier(
        n_estimators=100,
        learning_rate=0.05,
        max_depth=5,
        num_leaves=31,
        random_state=42,
        verbose=-1
    )
    
    # è¨“ç·´
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    # è©•ä¾¡
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    print(f"\nğŸ“Š è©•ä¾¡çµæœ:")
    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1 Score:  {f1:.4f}")
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1_weighted')
    
    print(f"\nğŸ”„ 5-Fold CV: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ“ˆ ç‰¹å¾´é‡é‡è¦åº¦ (Top 10):")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"  {row['feature']:30s}: {row['importance']*100:5.1f}%")
    
    return {
        'model': model,
        'feature_name': feature_name,
        'feature_cols': feature_cols,
        'metrics': {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1},
        'cv_f1_mean': cv_scores.mean(),
        'cv_f1_std': cv_scores.std(),
        'feature_importance': feature_importance,
        'y_pred': y_pred,
        'y_test': y_test
    }


def visualize_comparison(results_list, output_dir):
    """æ¯”è¼ƒçµæœã‚’å¯è¦–åŒ–"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‰¹å¾´é‡ã®åŠ¹æœæ¯”è¼ƒ', fontsize=16, fontweight='bold')
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
    
    for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
        ax = axes[idx // 2, idx % 2]
        names = [r['feature_name'] for r in results_list]
        values = [r['metrics'][metric] for r in results_list]
        
        bars = ax.bar(names, values, color=['#3498db', '#e74c3c'])
        ax.set_ylabel(metric_name, fontsize=12)
        ax.set_ylim(0, 1.1)
        ax.grid(axis='y', alpha=0.3)
        
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{value:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'engagement_metrics_comparison.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ {output_dir / 'engagement_metrics_comparison.png'}")
    plt.close()
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    engagement_result = next((r for r in results_list if 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæœ‰ã‚Š' in r['feature_name']), results_list[0])
    
    fig, ax = plt.subplots(figsize=(12, 8))
    top_features = engagement_result['feature_importance'].head(15)
    
    ax.barh(range(len(top_features)), top_features['importance'], color='#2ecc71')
    ax.set_yticks(range(len(top_features)))
    ax.set_yticklabels(top_features['feature'])
    ax.set_xlabel('é‡è¦åº¦', fontsize=12)
    ax.set_title(f'ç‰¹å¾´é‡é‡è¦åº¦ ({engagement_result["feature_name"]})', fontsize=14, fontweight='bold')
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ {output_dir / 'feature_importance.png'}")
    plt.close()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ”¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼")
    print("="*60)
    
    # æ—¢å­˜ã®çµ±ä¸€ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    data_path = project_root / "outputs" / "unified_model_v2" / "combined_labeled.csv"
    
    if not data_path.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        return
    
    print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {data_path}")
    df = pd.read_csv(data_path)
    
    # is_controversyã‚’labelã«å¤‰æ›
    if 'is_controversy' in df.columns:
        df['label'] = df['is_controversy']
    
    print(f"âœ“ {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    print(f"  ç‚ä¸Š: {(df['label']==1).sum()}ä»¶")
    print(f"  éç‚ä¸Š: {(df['label']==0).sum()}ä»¶")
    
    # è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
    y_train = train_df['label']
    y_test = test_df['label']
    
    results = []
    
    # ===============================================
    # å®Ÿé¨“1: åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ (7å€‹)
    # ===============================================
    BASE_FEATURES = [
        'volume', 'negative_rate', 'stance_against_rate',
        'stance_favor_rate', 'stance_neutral_rate',
        'delta_volume', 'delta_volume_rate'
    ]
    
    X_train_base = train_df[BASE_FEATURES].fillna(0).replace([np.inf, -np.inf], 0)
    X_test_base = test_df[BASE_FEATURES].fillna(0).replace([np.inf, -np.inf], 0)
    
    result_base = train_and_evaluate(
        X_train_base, y_train, X_test_base, y_test,
        "åŸºæœ¬ç‰¹å¾´é‡ (7å€‹)", BASE_FEATURES
    )
    results.append(result_base)
    
    # ===============================================
    # å®Ÿé¨“2: è¤‡åˆç‰¹å¾´é‡è¿½åŠ  (10å€‹)
    # ===============================================
    EXTENDED_FEATURES = BASE_FEATURES + [
        'flame_score', 'against_count', 'sentiment_polarity'
    ]
    
    X_train_ext = train_df[EXTENDED_FEATURES].fillna(0).replace([np.inf, -np.inf], 0)
    X_test_ext = test_df[EXTENDED_FEATURES].fillna(0).replace([np.inf, -np.inf], 0)
    
    result_ext = train_and_evaluate(
        X_train_ext, y_train, X_test_ext, y_test,
        "è¤‡åˆç‰¹å¾´é‡è¿½åŠ  (10å€‹)", EXTENDED_FEATURES
    )
    results.append(result_ext)
    
    # ===============================================
    # å®Ÿé¨“3: ä»®æƒ³ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‰¹å¾´é‡ (15å€‹)
    # ===============================================
    # æ³¨: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€
    # flame_scoreã‹ã‚‰æ¨å®šã—ãŸä»®æƒ³å€¤ã‚’ä½¿ç”¨
    print(f"\n{'='*60}")
    print("âš ï¸  æ³¨: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒæ¬ æã®ãŸã‚ã€ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã§è£œå®Œ")
    print(f"{'='*60}")
    
    df_virtual = df.copy()
    # ä»®æƒ³ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ = flame_score ã‚’åŸºæº–ã«ãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆ
    np.random.seed(42)
    df_virtual['avg_engagement'] = df['flame_score'] * np.random.uniform(0.5, 1.5, len(df))
    df_virtual['total_engagement'] = df_virtual['avg_engagement'] * df['volume']
    df_virtual['engagement_rate'] = df_virtual['total_engagement'] / (df['volume'] + 1)
    df_virtual['flame_engagement_score'] = df_virtual['total_engagement'] * df['negative_rate']
    df_virtual['against_engagement_score'] = df_virtual['total_engagement'] * df['stance_against_rate']
    
    train_virtual, test_virtual = train_test_split(df_virtual, test_size=0.2, random_state=42, stratify=df_virtual['label'])
    
    ENGAGEMENT_FEATURES = EXTENDED_FEATURES + [
        'avg_engagement', 'total_engagement', 'engagement_rate',
        'flame_engagement_score', 'against_engagement_score'
    ]
    
    X_train_eng = train_virtual[ENGAGEMENT_FEATURES].fillna(0).replace([np.inf, -np.inf], 0)
    X_test_eng = test_virtual[ENGAGEMENT_FEATURES].fillna(0).replace([np.inf, -np.inf], 0)
    y_train_eng = train_virtual['label']
    y_test_eng = test_virtual['label']
    
    result_eng = train_and_evaluate(
        X_train_eng, y_train_eng, X_test_eng, y_test_eng,
        "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæœ‰ã‚Š (15å€‹ãƒ»ä»®æƒ³)", ENGAGEMENT_FEATURES
    )
    results.append(result_eng)
    
    # å¯è¦–åŒ–
    output_dir = project_root / "outputs" / "engagement_comparison"
    visualize_comparison(results, output_dir)
    
    # JSONä¿å­˜
    comparison_data = {
        'baseline': {
            'feature_count': len(result_base['feature_cols']),
            'metrics': result_base['metrics'],
            'cv_f1': f"{result_base['cv_f1_mean']:.4f} Â± {result_base['cv_f1_std']:.4f}"
        },
        'extended': {
            'feature_count': len(result_ext['feature_cols']),
            'metrics': result_ext['metrics'],
            'cv_f1': f"{result_ext['cv_f1_mean']:.4f} Â± {result_ext['cv_f1_std']:.4f}"
        },
        'with_engagement_virtual': {
            'feature_count': len(result_eng['feature_cols']),
            'metrics': result_eng['metrics'],
            'cv_f1': f"{result_eng['cv_f1_mean']:.4f} Â± {result_eng['cv_f1_std']:.4f}",
            'note': 'ä»®æƒ³ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨'
        }
    }
    
    json_path = output_dir / 'comparison_results.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(comparison_data, f, indent=2, ensure_ascii=False)
    print(f"\nâœ“ {json_path}")
    
    # ã‚µãƒãƒªãƒ¼
    print(f"\n{'='*60}")
    print("ğŸ“Š æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    
    for i, result in enumerate(results, 1):
        print(f"\n{i}ï¸âƒ£  {result['feature_name']}")
        print(f"  F1 Score: {result['metrics']['f1']:.4f}")
        print(f"  CV F1:    {result['cv_f1_mean']:.4f} Â± {result['cv_f1_std']:.4f}")
    
    improvement = result_eng['metrics']['f1'] - result_ext['metrics']['f1']
    improvement_pct = improvement / result_ext['metrics']['f1'] * 100
    
    print(f"\nğŸ¯ æ”¹å–„åº¦ (è¤‡åˆâ†’ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ):")
    print(f"  F1 Score: {improvement:+.4f} ({improvement_pct:+.2f}%)")
    
    print(f"\nğŸ“ å‡ºåŠ›: {output_dir}")


if __name__ == "__main__":
    main()
