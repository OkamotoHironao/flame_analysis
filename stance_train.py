#!/usr/bin/env python3
"""
Stance Detection å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¥æœ¬èªBERTã‚’ä½¿ã£ãŸç«‹å ´åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
"""

import os
import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from tqdm import tqdm
import pandas as pd
from pathlib import Path

from stance_dataset import StanceDataset, load_dataset_from_csv


# ã‚·ãƒ¼ãƒ‰å›ºå®š
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


# Stance Detection ãƒ¢ãƒ‡ãƒ«
class StanceClassifier(nn.Module):
    """
    BERT + Classification Head
    """
    def __init__(self, model_name, num_labels=3, dropout=0.3):
        super(StanceClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output  # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºåŠ›
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits


def train_epoch(model, dataloader, optimizer, scheduler, device, scaler=None):
    """
    1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’
    """
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    progress_bar = tqdm(dataloader, desc="Training")
    
    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦å­¦ç¿’
        if scaler is not None:
            with torch.cuda.amp.autocast():
                logits = model(input_ids, attention_mask)
                loss = nn.CrossEntropyLoss()(logits, labels)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            logits = model(input_ids, attention_mask)
            loss = nn.CrossEntropyLoss()(logits, labels)
            loss.backward()
            optimizer.step()
        
        scheduler.step()
        
        # çµ±è¨ˆ
        total_loss += loss.item()
        _, predicted = torch.max(logits, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{100 * correct / total:.2f}%'
        })
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def evaluate(model, dataloader, device):
    """
    è©•ä¾¡
    """
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            logits = model(input_ids, attention_mask)
            loss = nn.CrossEntropyLoss()(logits, labels)
            
            total_loss += loss.item()
            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy


def main():
    print("=" * 60)
    print("Stance Detection å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    MODEL_NAME = "cl-tohoku/bert-base-japanese-v3"
    MAX_LENGTH = 128
    BATCH_SIZE = 32
    LEARNING_RATE = 2e-5
    EPOCHS = 5
    DROPOUT = 0.3
    TRAIN_RATIO = 0.8
    SEED = 42
    MODEL_DIR = "stance_model"
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»®ã®ãƒ‘ã‚¹ - å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦å¤‰æ›´ï¼‰
    TRAIN_CSV = "data/stance_train.csv"
    
    # ã‚·ãƒ¼ãƒ‰å›ºå®š
    set_seed(SEED)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # æ··åˆç²¾åº¦å­¦ç¿’ã®è¨­å®š
    use_amp = torch.cuda.is_available()
    scaler = torch.cuda.amp.GradScaler() if use_amp else None
    if use_amp:
        print("âš¡ æ··åˆç²¾åº¦å­¦ç¿’ã‚’æœ‰åŠ¹åŒ–")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = StanceClassifier(MODEL_NAME, num_labels=3, dropout=DROPOUT)
    model.to(device)
    print("âœ“ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {TRAIN_CSV}")
    
    # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    if not Path(TRAIN_CSV).exists():
        print("âš ï¸  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™...")
        Path("data").mkdir(exist_ok=True)
        
        sample_data = {
            'content': [
                'æ¾æœ¬äººå¿—ã¯æœ€ä½ã ã€‚è¨±ã›ãªã„ã€‚',
                'æ€§åŠ å®³ã¯çµ¶å¯¾ã«è¨±ã•ã‚Œãªã„è¡Œç‚ºã ã€‚',
                'æ¾æœ¬äººå¿—ã‚’æ”¯æŒã—ã¾ã™ã€‚é ‘å¼µã£ã¦ã»ã—ã„ã€‚',
                'æ¾æœ¬äººå¿—ã®æ‰èƒ½ã¯ç´ æ™´ã‚‰ã—ã„ã€‚',
                'æ¾æœ¬äººå¿—ãŒå¾©å¸°ã—ãŸã¨ã„ã†ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ãŸã€‚',
                'äº‹å®Ÿé–¢ä¿‚ã‚’å†·é™ã«è¦‹ã‚‹ã¹ãã ã€‚',
            ] * 20,  # 120ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«
            'label': ['AGAINST', 'AGAINST', 'FAVOR', 'FAVOR', 'NEUTRAL', 'NEUTRAL'] * 20
        }
        pd.DataFrame(sample_data).to_csv(TRAIN_CSV, index=False)
        print(f"âœ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ: {TRAIN_CSV}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    full_dataset = load_dataset_from_csv(TRAIN_CSV, tokenizer, MAX_LENGTH)
    print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(full_dataset)}ä»¶")
    
    # Train/Valåˆ†å‰²
    train_size = int(TRAIN_RATIO * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    print(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_size}ä»¶")
    print(f"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {val_size}ä»¶")
    
    # DataLoaderä½œæˆ
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    total_steps = len(train_loader) * EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),
        num_training_steps=total_steps
    )
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print(f"\nğŸš€ å­¦ç¿’é–‹å§‹ï¼ˆ{EPOCHS}ã‚¨ãƒãƒƒã‚¯ï¼‰")
    best_val_acc = 0
    
    for epoch in range(EPOCHS):
        print(f"\nã€ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{EPOCHS}ã€‘")
        
        # è¨“ç·´
        train_loss, train_acc = train_epoch(
            model, train_loader, optimizer, scheduler, device, scaler
        )
        print(f"è¨“ç·´ - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%")
        
        # è©•ä¾¡
        val_loss, val_acc = evaluate(model, val_loader, device)
        print(f"æ¤œè¨¼ - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            print(f"âœ¨ æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆAcc: {val_acc:.2f}%ï¼‰")
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            Path(MODEL_DIR).mkdir(exist_ok=True)
            torch.save(model.state_dict(), f"{MODEL_DIR}/best_model.pth")
            tokenizer.save_pretrained(MODEL_DIR)
            
            # è¨­å®šä¿å­˜
            config = {
                'model_name': MODEL_NAME,
                'max_length': MAX_LENGTH,
                'dropout': DROPOUT,
                'num_labels': 3
            }
            import json
            with open(f"{MODEL_DIR}/config.json", 'w') as f:
                json.dump(config, f, indent=2)
    
    print("\n" + "=" * 60)
    print(f"âœ… å­¦ç¿’å®Œäº†ï¼ ãƒ™ã‚¹ãƒˆç²¾åº¦: {best_val_acc:.2f}%")
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {MODEL_DIR}/")
    print("=" * 60)


if __name__ == "__main__":
    main()
