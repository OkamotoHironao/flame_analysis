#!/usr/bin/env python3
"""
å…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§çµ±åˆãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

6ã¤ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãã‚Œãã‚Œã§çµ±åˆãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€æ€§èƒ½ã‚’æ¯”è¼ƒ
- CatBoost
- XGBoost
- LightGBM
- Random Forest
- SVM (RBF)
- Logistic Regression

Usage:
    python train_all_unified_models.py
"""

import sys
from pathlib import Path
import json
import time
from datetime import datetime

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix,
    precision_recall_curve
)
from sklearn.inspection import permutation_importance
import joblib

# ãƒ¢ãƒ‡ãƒ«
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# 16ç‰¹å¾´é‡ï¼ˆcompare_all_models.pyã¨åŒã˜ï¼‰
FEATURES = [
    'volume',
    'negative_rate',
    'stance_against_rate',
    'stance_favor_rate',
    'stance_neutral_rate',
    'delta_volume',
    'delta_volume_rate',
    'flame_score',
    'against_count',
    'sentiment_polarity',
    'delta_negative_rate',
    'delta_against_rate',
    'sentiment_avg_score',
    'stance_against_mean',
    'stance_favor_mean',
    'stance_neutral_mean',
]


def load_unified_data():
    """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    data_path = Path("outputs/unified_model_v2/combined_labeled.csv")
    
    if not data_path.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        return None, None
    
    df = pd.read_csv(data_path)
    print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
    print(f"  ç‚ä¸Š: {(df['is_controversy'] == 1).sum()}ä»¶")
    print(f"  éç‚ä¸Š: {(df['is_controversy'] == 0).sum()}ä»¶")
    
    # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«
    available_features = [f for f in FEATURES if f in df.columns]
    missing_features = [f for f in FEATURES if f not in df.columns]
    
    if missing_features:
        print(f"âš ï¸ æ¬ æç‰¹å¾´é‡: {missing_features}")
    
    X = df[available_features]
    y = df['is_controversy']
    
    print(f"âœ“ ä½¿ç”¨ç‰¹å¾´é‡: {len(available_features)}å€‹")
    
    return X, y


def get_model_configs():
    """å„ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’è¿”ã™"""
    configs = {
        'CatBoost': {
            'model': CatBoostClassifier(
                iterations=100,
                depth=3,
                learning_rate=0.1,
                loss_function='Logloss',
                random_seed=42,
                verbose=False
            ),
            'needs_scaling': False
        },
        'XGBoost': {
            'model': XGBClassifier(
                n_estimators=100,
                max_depth=3,
                learning_rate=0.05,
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss',
                min_child_weight=3,
                subsample=0.8,
                colsample_bytree=0.8
            ),
            'needs_scaling': False
        },
        'LightGBM': {
            'model': LGBMClassifier(
                n_estimators=100,
                max_depth=3,
                learning_rate=0.1,
                random_state=42,
                verbose=-1
            ),
            'needs_scaling': False
        },
        'Random Forest': {
            'model': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            ),
            'needs_scaling': False
        },
        'SVM (RBF)': {
            'model': SVC(
                C=10,
                kernel='rbf',
                gamma='scale',
                probability=True,
                random_state=42
            ),
            'needs_scaling': True
        },
        'Logistic Regression': {
            'model': LogisticRegression(
                C=1.0,
                max_iter=1000,
                random_state=42
            ),
            'needs_scaling': True
        }
    }
    
    return configs


def find_optimal_threshold(model, X_test, y_test, scaler=None):
    """æœ€é©ãªé–¾å€¤ã‚’æ¢ç´¢"""
    if scaler is not None:
        X_test_scaled = scaler.transform(X_test)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, y_pred_proba)
    
    # F1ã‚¹ã‚³ã‚¢æœ€å¤§ã®é–¾å€¤ã‚’æ¢ã™
    best_threshold = 0.5
    best_f1 = 0
    
    for i, threshold in enumerate(thresholds):
        if i < len(recall_vals) - 1:
            r = recall_vals[i]
            p = precision_vals[i]
            
            if p > 0 and r > 0:
                f1 = 2 * p * r / (p + r)
                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = threshold
    
    return best_threshold


def train_and_evaluate_model(model_name, model, X_train, y_train, X_test, y_test, needs_scaling=False):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦è©•ä¾¡"""
    print(f"\n{'='*70}")
    print(f"ğŸ”§ {model_name}")
    print(f"{'='*70}")
    
    scaler = None
    if needs_scaling:
        print("  ğŸ“Š ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–ä¸­...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
    else:
        X_train_scaled = X_train
        X_test_scaled = X_test
    
    # è¨“ç·´
    print(f"  ğŸ“ è¨“ç·´ä¸­...")
    start_time = time.time()
    model.fit(X_train_scaled, y_train)
    train_time = time.time() - start_time
    print(f"  âœ“ è¨“ç·´å®Œäº† ({train_time:.2f}ç§’)")
    
    # é–¾å€¤æœ€é©åŒ–
    print(f"  ğŸ¯ æœ€é©é–¾å€¤ã‚’æ¢ç´¢ä¸­...")
    threshold = find_optimal_threshold(model, X_test, y_test, scaler if needs_scaling else None)
    print(f"  âœ“ æœ€é©é–¾å€¤: {threshold:.4f}")
    
    # äºˆæ¸¬
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    y_pred = (y_pred_proba >= threshold).astype(int)
    
    # è©•ä¾¡
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1': f1_score(y_test, y_pred, zero_division=0),
        'roc_auc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else None
    }
    
    print(f"\n  ğŸ“Š è©•ä¾¡çµæœ:")
    print(f"    Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"    Precision: {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)")
    print(f"    Recall:    {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)")
    print(f"    F1 Score:  {metrics['f1']:.4f} ({metrics['f1']*100:.2f}%)")
    if metrics['roc_auc']:
        print(f"    ROC-AUC:   {metrics['roc_auc']:.4f}")
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    print(f"\n  ğŸ”„ 5-Fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä¸­...")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = {
        'accuracy': cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy'),
        'f1': cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='f1'),
        'roc_auc': cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='roc_auc')
    }
    
    print(f"    CV Accuracy: {cv_scores['accuracy'].mean():.4f} Â± {cv_scores['accuracy'].std():.4f}")
    print(f"    CV F1:       {cv_scores['f1'].mean():.4f} Â± {cv_scores['f1'].std():.4f}")
    print(f"    CV ROC-AUC:  {cv_scores['roc_auc'].mean():.4f} Â± {cv_scores['roc_auc'].std():.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = None
    if hasattr(model, 'feature_importances_'):
        feature_importance = dict(zip(X_train.columns, model.feature_importances_))
    elif hasattr(model, 'coef_'):
        # Logistic Regressionã®ä¿‚æ•°
        feature_importance = dict(zip(X_train.columns, np.abs(model.coef_[0])))
    else:
        # SVMãªã©ã€ç›´æ¥çš„ãªç‰¹å¾´é‡é‡è¦åº¦ãŒãªã„å ´åˆã¯permutation importanceã‚’è¨ˆç®—
        print(f"  ğŸ” Permutation Importanceè¨ˆç®—ä¸­...")
        perm_result = permutation_importance(
            model, X_test_scaled, y_test, 
            n_repeats=10, random_state=42, n_jobs=-1
        )
        feature_importance = dict(zip(X_train.columns, perm_result.importances_mean))
        print(f"  âœ“ Permutation Importanceè¨ˆç®—å®Œäº†")
    
    return {
        'model': model,
        'scaler': scaler,
        'threshold': threshold,
        'metrics': metrics,
        'cv_scores': {
            'accuracy_mean': cv_scores['accuracy'].mean(),
            'accuracy_std': cv_scores['accuracy'].std(),
            'f1_mean': cv_scores['f1'].mean(),
            'f1_std': cv_scores['f1'].std(),
            'roc_auc_mean': cv_scores['roc_auc'].mean(),
            'roc_auc_std': cv_scores['roc_auc'].std()
        },
        'train_time': train_time,
        'feature_importance': feature_importance
    }


def save_model(model_name, result, X_train, output_dir):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
    model_dir = Path(output_dir) / model_name.replace(' ', '_').replace('(', '').replace(')', '')
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    joblib.dump(result['model'], model_dir / 'model.pkl')
    
    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜ï¼ˆã‚ã‚‹å ´åˆï¼‰
    if result['scaler'] is not None:
        joblib.dump(result['scaler'], model_dir / 'scaler.pkl')
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆfloat32ã‚’floatã«å¤‰æ›ï¼‰
    def convert_to_python_types(obj):
        """numpy/pandaså‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›"""
        if isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_python_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_python_types(item) for item in obj]
        return obj
    
    metadata = {
        'model_name': model_name,
        'created_at': datetime.now().isoformat(),
        'threshold': float(result['threshold']),
        'features': list(X_train.columns),
        'metrics': {
            'test': convert_to_python_types(result['metrics']),
            'cv': convert_to_python_types(result['cv_scores'])
        },
        'train_time': float(result['train_time']),
        'feature_importance': convert_to_python_types(result['feature_importance']) if result['feature_importance'] else None
    }
    
    with open(model_dir / 'metadata.json', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"  âœ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_dir}")


def main():
    print("=" * 70)
    print("ğŸ”¥ å…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çµ±åˆãƒ¢ãƒ‡ãƒ«è¨“ç·´")
    print("=" * 70)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    X, y = load_unified_data()
    
    if X is None:
        return
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    print("\nâœ‚ï¸ ãƒ‡ãƒ¼ã‚¿åˆ†å‰²")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"  è¨“ç·´: {len(X_train)}ä»¶, ãƒ†ã‚¹ãƒˆ: {len(X_test)}ä»¶")
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®šå–å¾—
    configs = get_model_configs()
    
    # å„ãƒ¢ãƒ‡ãƒ«ã§è¨“ç·´
    results = {}
    output_dir = Path("outputs/unified_models_comparison")
    
    for model_name, config in configs.items():
        result = train_and_evaluate_model(
            model_name,
            config['model'],
            X_train,
            y_train,
            X_test,
            y_test,
            needs_scaling=config['needs_scaling']
        )
        
        results[model_name] = result
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        save_model(model_name, result, X_train, output_dir)
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\n{'='*70}")
    print("ğŸ“Š å…¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚µãƒãƒªãƒ¼")
    print(f"{'='*70}\n")
    
    # F1ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    sorted_results = sorted(results.items(), key=lambda x: x[1]['metrics']['f1'], reverse=True)
    
    print(f"{'é †ä½':<6} {'ãƒ¢ãƒ‡ãƒ«':<25} {'F1':<10} {'Accuracy':<10} {'è¨“ç·´æ™‚é–“':<10}")
    print("-" * 70)
    
    for rank, (model_name, result) in enumerate(sorted_results, 1):
        icon = "ğŸ†" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"{rank}ä½"
        print(f"{icon:<6} {model_name:<25} {result['metrics']['f1']:.4f}    {result['metrics']['accuracy']:.4f}    {result['train_time']:.2f}ç§’")
    
    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    def convert_to_python_types(obj):
        """numpy/pandaså‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›"""
        if isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_python_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_python_types(item) for item in obj]
        return obj
    
    summary = {
        'created_at': datetime.now().isoformat(),
        'total_models': len(results),
        'best_model': sorted_results[0][0],
        'best_f1': float(sorted_results[0][1]['metrics']['f1']),
        'results': {
            name: {
                'metrics': convert_to_python_types(res['metrics']),
                'cv_scores': convert_to_python_types(res['cv_scores']),
                'threshold': float(res['threshold']),
                'train_time': float(res['train_time'])
            }
            for name, res in results.items()
        }
    }
    
    with open(output_dir / 'summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ ç·åˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_dir / 'summary.json'}")
    print(f"\nğŸ‰ å…¨ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†ï¼")


if __name__ == '__main__':
    main()
