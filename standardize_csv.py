#!/usr/bin/env python3
"""
CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±ä¸€åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ©Ÿèƒ½:
1. å„ãƒˆãƒ”ãƒƒã‚¯ã®ã‚ªãƒªã‚¸ãƒŠãƒ«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
2. ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’å‰Šé™¤
3. æ”¹è¡Œã‚„ã‚«ãƒ³ãƒã‚’å«ã‚€contentåˆ—ã‚’é©åˆ‡ã«å‡¦ç†
4. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’JSTã«çµ±ä¸€
5. é‡è¤‡ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å‰Šé™¤
6. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ¥é€”JSONä¿å­˜

å‡ºåŠ›å½¢å¼:
- data/standardized/{topic}.csv: çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®CSV
- data/standardized/{topic}_meta.json: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå–å¾—æœŸé–“ã€ä»¶æ•°ç­‰ï¼‰

Usage:
    python standardize_csv.py                    # å…¨ãƒˆãƒ”ãƒƒã‚¯å‡¦ç†
    python standardize_csv.py æ¾æœ¬äººå¿—           # ç‰¹å®šãƒˆãƒ”ãƒƒã‚¯ã®ã¿
    python standardize_csv.py --list             # åˆ©ç”¨å¯èƒ½ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§
"""

import argparse
import json
import re
import sys
from datetime import datetime
from pathlib import Path

import pandas as pd
from zoneinfo import ZoneInfo

# è¨­å®š
BASE_DIR = Path(__file__).parent
ORIGINAL_DIR = BASE_DIR / "data" / "original"
OUTPUT_DIR = BASE_DIR / "data" / "standardized"

# çµ±ä¸€ã‚«ãƒ©ãƒ å®šç¾©
STANDARD_COLUMNS = [
    "timestamp",          # datetime (JST)
    "tweet_id",           # str (ID)
    "url",                # str
    "content",            # str (ãƒ†ã‚­ã‚¹ãƒˆ)
    "user_id",            # str
    "user_name",          # str
    "reply_count",        # int
    "retweet_count",      # int
    "like_count",         # int
]


def discover_topics():
    """åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ«ãƒ€åï¼‰ã‚’æ¤œå‡º"""
    topics = []
    if ORIGINAL_DIR.exists():
        for item in ORIGINAL_DIR.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                topics.append(item.name)
    return sorted(topics)


def parse_comment_metadata(file_path):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    metadata = {
        "query": None,
        "collected_at": None,
        "original_count": None,
        "source_file": str(file_path.name)
    }
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.startswith('#'):
                break
            line = line.strip()
            if 'æ¤œç´¢ã‚¯ã‚¨ãƒª:' in line:
                metadata["query"] = line.split('æ¤œç´¢ã‚¯ã‚¨ãƒª:')[1].strip()
            elif 'å–å¾—æ—¥æ™‚:' in line:
                metadata["collected_at"] = line.split('å–å¾—æ—¥æ™‚:')[1].strip()
            elif 'å–å¾—ä»¶æ•°:' in line:
                try:
                    metadata["original_count"] = int(line.split('å–å¾—ä»¶æ•°:')[1].strip())
                except ValueError:
                    pass
    
    return metadata


def read_csv_with_comments(file_path):
    """
    ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦CSVã‚’èª­ã¿è¾¼ã‚€
    æ”¹è¡Œã‚’å«ã‚€contentåˆ—ã‚‚é©åˆ‡ã«å‡¦ç†
    """
    try:
        # ã¾ãšã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦èª­ã¿è¾¼ã¿
        df = pd.read_csv(
            file_path,
            comment='#',
            on_bad_lines='warn',
            encoding='utf-8'
        )
        return df
    except Exception as e:
        print(f"  âš ï¸ æ¨™æº–èª­ã¿è¾¼ã¿å¤±æ•—ã€è¡Œå˜ä½å‡¦ç†ã‚’è©¦è¡Œ: {e}")
        return read_csv_line_by_line(file_path)


def read_csv_line_by_line(file_path):
    """
    CSVã‚’è¡Œå˜ä½ã§èª­ã¿è¾¼ã¿ï¼ˆãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
    """
    rows = []
    header = None
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’é™¤å»
    lines = []
    for line in content.split('\n'):
        if not line.startswith('#'):
            lines.append(line)
    content = '\n'.join(lines)
    
    # pandas ã§å†ãƒ‘ãƒ¼ã‚¹
    from io import StringIO
    try:
        df = pd.read_csv(
            StringIO(content),
            on_bad_lines='skip',
            encoding='utf-8'
        )
        return df
    except Exception as e:
        print(f"  âŒ CSVèª­ã¿è¾¼ã¿å®Œå…¨å¤±æ•—: {e}")
        return pd.DataFrame()


def convert_to_jst(timestamp_str):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’JSTã«å¤‰æ›"""
    try:
        # ISO 8601å½¢å¼ (2023-01-29T14:25:37.000Z)
        if 'T' in str(timestamp_str) and str(timestamp_str).endswith('Z'):
            dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            return dt.astimezone(ZoneInfo('Asia/Tokyo'))
        # ãã®ä»–ã®å½¢å¼
        dt = pd.to_datetime(timestamp_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=ZoneInfo('UTC'))
        return dt.astimezone(ZoneInfo('Asia/Tokyo'))
    except Exception:
        return None


def standardize_dataframe(df):
    """DataFrameã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›"""
    std_df = pd.DataFrame()
    
    # ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°
    column_mapping = {
        'date': 'timestamp',
        'id': 'tweet_id',
        'url': 'url',
        'content': 'content',
        'user': 'user_id',
        'user_displayname': 'user_name',
        'reply_count': 'reply_count',
        'retweet_count': 'retweet_count',
        'like_count': 'like_count',
    }
    
    for old_col, new_col in column_mapping.items():
        if old_col in df.columns:
            std_df[new_col] = df[old_col]
        else:
            std_df[new_col] = None
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’JSTã«å¤‰æ›
    std_df['timestamp'] = std_df['timestamp'].apply(convert_to_jst)
    
    # æ•°å€¤åˆ—ã®å‹å¤‰æ›
    for col in ['reply_count', 'retweet_count', 'like_count']:
        if col in std_df.columns:
            std_df[col] = pd.to_numeric(std_df[col], errors='coerce').fillna(0).astype(int)
    
    # tweet_idã‚’æ–‡å­—åˆ—ã«
    std_df['tweet_id'] = std_df['tweet_id'].astype(str)
    
    # content ã®æ”¹è¡Œã‚’æ­£è¦åŒ–ï¼ˆ\n â†’ ç©ºç™½ï¼‰
    std_df['content'] = std_df['content'].fillna('').astype(str).str.replace('\n', ' ').str.replace('\r', '')
    
    return std_df


def process_topic(topic_name, force=False):
    """
    ãƒˆãƒ”ãƒƒã‚¯ã®å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
    
    Args:
        topic_name: ãƒˆãƒ”ãƒƒã‚¯åï¼ˆãƒ•ã‚©ãƒ«ãƒ€åï¼‰
        force: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã™ã‚‹ã‹
    
    Returns:
        dict: å‡¦ç†çµæœ
    """
    topic_dir = ORIGINAL_DIR / topic_name
    output_csv = OUTPUT_DIR / f"{topic_name}.csv"
    output_meta = OUTPUT_DIR / f"{topic_name}_meta.json"
    
    print(f"\n{'='*60}")
    print(f"ğŸ“ ãƒˆãƒ”ãƒƒã‚¯: {topic_name}")
    print(f"{'='*60}")
    
    if not topic_dir.exists():
        print(f"âŒ ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {topic_dir}")
        return {"status": "error", "message": "ãƒ•ã‚©ãƒ«ãƒ€ãªã—"}
    
    if output_csv.exists() and not force:
        print(f"â­ï¸ æ—¢ã«å­˜åœ¨ã—ã¾ã™ï¼ˆ--forceã§ä¸Šæ›¸ãï¼‰: {output_csv}")
        return {"status": "skipped", "message": "æ—¢å­˜"}
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    csv_files = list(topic_dir.glob("*.csv"))
    if not csv_files:
        print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"status": "error", "message": "CSVãªã—"}
    
    print(f"ğŸ“‚ {len(csv_files)}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º")
    
    # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    all_dfs = []
    all_metadata = []
    
    for csv_file in sorted(csv_files):
        print(f"  ğŸ“„ {csv_file.name}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        meta = parse_comment_metadata(csv_file)
        all_metadata.append(meta)
        
        # CSVèª­ã¿è¾¼ã¿
        df = read_csv_with_comments(csv_file)
        if df.empty:
            print(f"    âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—")
            continue
        
        print(f"    âœ“ {len(df)}ä»¶")
        all_dfs.append(df)
    
    if not all_dfs:
        print(f"âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return {"status": "error", "message": "ãƒ‡ãƒ¼ã‚¿ãªã—"}
    
    # çµåˆ
    combined_df = pd.concat(all_dfs, ignore_index=True)
    print(f"\nğŸ“Š çµåˆçµæœ: {len(combined_df)}ä»¶")
    
    # çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
    std_df = standardize_dataframe(combined_df)
    
    # ç„¡åŠ¹ãªè¡Œã‚’å‰Šé™¤ï¼ˆtimestampãŒNoneï¼‰
    valid_count_before = len(std_df)
    std_df = std_df.dropna(subset=['timestamp'])
    if len(std_df) < valid_count_before:
        print(f"  âš ï¸ ç„¡åŠ¹ãªæ—¥ä»˜ã®è¡Œã‚’é™¤å¤–: {valid_count_before - len(std_df)}ä»¶")
    
    # é‡è¤‡å‰Šé™¤ï¼ˆtweet_idãƒ™ãƒ¼ã‚¹ï¼‰
    dup_count = std_df.duplicated(subset=['tweet_id']).sum()
    std_df = std_df.drop_duplicates(subset=['tweet_id'])
    if dup_count > 0:
        print(f"  âœ“ é‡è¤‡å‰Šé™¤: {dup_count}ä»¶")
    
    # æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆ
    std_df = std_df.sort_values('timestamp').reset_index(drop=True)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # CSVä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜ä¸­: {output_csv}")
    std_df.to_csv(output_csv, index=False)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ
    period_start = std_df['timestamp'].min()
    period_end = std_df['timestamp'].max()
    
    meta_summary = {
        "topic": topic_name,
        "created_at": datetime.now().isoformat(),
        "total_tweets": len(std_df),
        "period": {
            "start": period_start.isoformat() if period_start else None,
            "end": period_end.isoformat() if period_end else None,
        },
        "source_files": len(csv_files),
        "queries": list(set(m["query"] for m in all_metadata if m["query"])),
        "columns": STANDARD_COLUMNS,
    }
    
    with open(output_meta, 'w', encoding='utf-8') as f:
        json.dump(meta_summary, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {output_meta}")
    
    # çµ±è¨ˆè¡¨ç¤º
    print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
    print(f"  æœŸé–“: {period_start} ã€œ {period_end}")
    print(f"  ç·ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {len(std_df)}")
    print(f"  æ—¥æ•°: {(period_end - period_start).days + 1 if period_start and period_end else 'N/A'}æ—¥")
    
    # æ™‚é–“åˆ†å¸ƒã®ç¢ºèª
    hourly_dist = std_df.groupby(std_df['timestamp'].dt.hour).size()
    print(f"  æ™‚é–“å¸¯åˆ†å¸ƒ (JST):")
    for hour in range(0, 24, 6):
        count = hourly_dist.get(hour, 0)
        print(f"    {hour:02d}:00 - {count}ä»¶")
    
    return {
        "status": "success",
        "total_tweets": len(std_df),
        "output_csv": str(output_csv),
    }


def main():
    parser = argparse.ArgumentParser(
        description="CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±ä¸€åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¾‹:
  python standardize_csv.py              # å…¨ãƒˆãƒ”ãƒƒã‚¯å‡¦ç†
  python standardize_csv.py æ¾æœ¬äººå¿—     # ç‰¹å®šãƒˆãƒ”ãƒƒã‚¯ã®ã¿
  python standardize_csv.py --list       # åˆ©ç”¨å¯èƒ½ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§
  python standardize_csv.py --force      # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã
        """
    )
    parser.add_argument('topic', nargs='?', help='å‡¦ç†ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯å')
    parser.add_argument('--list', action='store_true', help='åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤º')
    parser.add_argument('--force', '-f', action='store_true', help='æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã')
    parser.add_argument('--all', '-a', action='store_true', help='å…¨ãƒˆãƒ”ãƒƒã‚¯ã‚’å‡¦ç†')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ“‹ CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±ä¸€åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’å–å¾—
    topics = discover_topics()
    
    if args.list:
        print(f"\nåˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯ ({len(topics)}ä»¶):")
        for topic in topics:
            csv_count = len(list((ORIGINAL_DIR / topic).glob("*.csv")))
            print(f"  - {topic} ({csv_count} files)")
        return 0
    
    if not topics:
        print("âŒ ã‚¨ãƒ©ãƒ¼: data/original/ ã«ãƒˆãƒ”ãƒƒã‚¯ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚Šã¾ã›ã‚“")
        return 1
    
    # å‡¦ç†å¯¾è±¡ã‚’æ±ºå®š
    if args.topic:
        if args.topic not in topics:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒˆãƒ”ãƒƒã‚¯ '{args.topic}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print(f"åˆ©ç”¨å¯èƒ½: {', '.join(topics)}")
            return 1
        target_topics = [args.topic]
    elif args.all:
        target_topics = topics
    else:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–é¸æŠ
        print(f"\nåˆ©ç”¨å¯èƒ½ãªãƒˆãƒ”ãƒƒã‚¯:")
        for i, topic in enumerate(topics, 1):
            csv_count = len(list((ORIGINAL_DIR / topic).glob("*.csv")))
            print(f"  {i}. {topic} ({csv_count} files)")
        print(f"  0. å…¨ã¦å‡¦ç†")
        
        try:
            choice = input("\nå‡¦ç†ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’é¸æŠ (ç•ªå·): ").strip()
            if choice == '0':
                target_topics = topics
            else:
                idx = int(choice) - 1
                if 0 <= idx < len(topics):
                    target_topics = [topics[idx]]
                else:
                    print("âŒ ç„¡åŠ¹ãªé¸æŠ")
                    return 1
        except (ValueError, EOFError):
            print("âŒ ç„¡åŠ¹ãªå…¥åŠ›")
            return 1
    
    # å‡¦ç†å®Ÿè¡Œ
    results = {}
    for topic in target_topics:
        result = process_topic(topic, force=args.force)
        results[topic] = result
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“Š å‡¦ç†çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    for topic, result in results.items():
        status_emoji = {"success": "âœ…", "skipped": "â­ï¸", "error": "âŒ"}.get(result["status"], "â“")
        if result["status"] == "success":
            print(f"{status_emoji} {topic}: {result['total_tweets']}ä»¶")
        else:
            print(f"{status_emoji} {topic}: {result.get('message', result['status'])}")
    
    print("\n" + "=" * 60)
    print("âœ… å®Œäº†ï¼çµ±ä¸€åŒ–ã•ã‚ŒãŸCSVã¯ data/standardized/ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print("=" * 60)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
